---
title: Proyecto de Clasificación de Voces de Personas
author:
  - name: Luis Albacete Caballero
    email: alcaluis@alumni.uv.es
    affiliation: UV
  - name: Julio García Bustos
    email: jugarbus@alumni.uv.es
    affiliation: UV
  - name: Gabriel Ivars Asensio
    email: gaia2@alumni.uv.es
    affiliation: UV
  - name: Noé López García
    email: nologar@alumni.uv.es
    affiliation: UV
  - name: José Miguel Palazón Caballero
    email: jomipaca@alumni.uv.es
    affiliation: UV
  - name: Joan Pedro Bruixola
    email: jopebrui@alumni.uv.es
    affiliation: UV
address:
  - code: UV
    organization: Universitat de València
    addressline: Avinguda de l'Universitat
    city: Burjassot
    state: Valencia
    postcode: 46100
    country: España
abstract: |
  La clasificación de voces de personas es un tema latente en el análisis de señales. Requiriendo extraer particularidades y características de la voz como señal. A lo largo de este trabajo obtendremos distintas características utilizando librerias o implementando nosotros la propia extracción de la característica. Bajo la finalidad de poder clasificar audios dependiendo del género del locutor.
keywords: 
  - Voces
  - Características
  - Características de la voz
  - Clasificación
  - Aprendizaje Máquina
  - Análisis de Señales
  - Identificación voces
date: "`r Sys.Date()`"
linenumbers: false
numbersections: true
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
---

# Captación de audio

A fin de poder extraer las características de los audios, requerimos en primera instancia de los audios en sí. Para ello hemos captado 70 audios, grabados por 10 compañeros del Máster de Ciencia de Datos.

Para agilizar las labores de preparación del contenido, los audios fueron grabados bajo las mismas condiciones, desde software y hardware de captación de audio como la sala de grabación e instrucciones para la grabación.

Aprovechar para agredecer a todos los colaboradores su intervención.

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(#warning=FALSE,
                      out.width = "80%",
                      fig.align = "center",
                      echo = FALSE)

# Cargar librerías
librerias <- c("tuneR",      # Lectura de archivos WAV
               "seewave",    # Manipulación mediante wavelets
               "plotly",     # Gráficos 3D
               "phonTools",  # Libreria para obtención de Formants
               "dplyr",      # Manipulacion de datos
               "kableExtra", # Kable para ajutar las tablas
               "wavelets",   # Libreria para manipulación con Wavelets
               "signal",     # Funciones de procesado de señales
               "ggplot2",    # Librería de gráficos
               "ggpubr",     # Mostrar múltiples ggplots juntos
               "lomb"
              )
pacman::p_load(char = librerias, warn.conflicts=FALSE)

# Cargar utilidades
source("../Utils/extraccion_caracteristicas.R")
source("../Utils/carga_datos.R")
```

```{r limpieza_audios}
# Cargar datos + Metadatos
carga <- carga_audios("../Data/Audios/")
audios <- carga[[1]]
meta_audios <- carga[[2]]

# 1. Eliminación ruido
audios_lim <- limpieza_senales(audios)

# 2. Normalización
audios <- normalizacion(audios_lim, norm_amplitud=FALSE)
audios_norm <- normalizacion(audios_lim)

# 3. DataFrame características
df_caract <- data.frame("id_audio" = meta_audios$id_audio)
```

# Característica de la voz

En el desarrollo de este apartado probaremos distintas técnicas de extracción de características.

## Mel Frecuency Cepstral Coefficients (MFCC)

Esta característica es una de las más comunes para reconocimiento de voces. Combina el análisis de cepstrums con una escala perceptual de las frecuencias.

Los coeficientes de Mel se basan en la percepción humana utilizando una escala linealmente separada para los valores de la señal inferior a 1KHz y una logarítmica en caso de ser superior.

El cálculo de los coeficientes se constituye de los siguientes pasos:

1. **Pre-emphasis**. El primer paso consiste en pasar la señal por un filtro paso alto. En este se incrementará la energía de las altas frecuencias. Utilizando la siguiente fórmula: $y(n) = x(n) - a * x(n - 1)$ con valores de $a$ entre 0.9 y 1.
2. **Frame blocking**. Consiste en la división de la señal en frames de entre 20 y 30 ms.
3. **Windowing**. Seguidamente se aplica un efecto de "windowing" de manera que los bordes de la señal (frame) sean suavizados (más cercano a 0 en los extremos). La función de ventana que se utiliza es la de Hamming.
4. **Discrete Fourier Transform**. Como deseamos recoger las energías en el dominio de las frecuencias debemos hacer un cambio de dominio mediante la DFT.
5. **Mel-Filter**. Se suma las energías de las componentes espectrales por cada escala de Mel. Al resultado del filtrado se escalará logarítmicamente.
5. **Discrete Cousine Transform**. Finalmente se aplicará la siguiente transformada: $C(n) = \sum Ek * cos(n * (k-0.5) * \pi / 40)$ para volver al dominio del tiempo. Obteniendo los coeficientes de Mel que recogen la energía total escalada por cada banda de frecuencia por cada intervalo.

```{r extraccion_caract_mfcc, warning=FALSE}
mel_coefs <- list()

id <- 1
for (audio in audios_norm) {
  # Calculo de coeficientes y media por cada audio
  coefs <- colMeans(melfcc(audio,
                           minfreq = 50,
                           maxfreq = 4000,
                           sr = audio@samp.rate,
                           wintime = 0.025, usecmp = TRUE))
  
  # De los 12 coeficientes haremos agrupaciones:
  mel_coefs[[id]] <- c(coefs[1], mean(coefs[2:3]),
                       mean(coefs[4:6]), mean(coefs[7:12]))
  id <- id + 1
}

mel_mat <- matrix(unlist(mel_coefs), byrow=TRUE, ncol=4)
colnames(mel_mat) <- c("MEL_COE1", "MEL_COE2_3", "MEL_COE4_6",
                       "MEL_COE7_12")

df_caract <- cbind(df_caract, mel_mat)
```

Agruparemos los coeficientes para un análisis más conciso basandonos en el concepto del oído humano. Escalando poco a poco las agrupaciones. En la siguiente gráfica veremos los coeficientes entre hombres y mujeres para las 5 primeras pistas de audio.

```{r analisis_mfcc}
df_analisis_mel <- df_caract %>%
  left_join(meta_audios[c("genero", "pista", "id_audio")], by = "id_audio") %>%
  dplyr::filter(pista %in% paste0("0", c(1:5)))

p_coe1 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE1, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [1]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe2 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE2_3, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [2:3]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe3 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE4_6, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [4:6]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe4 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE7_12, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [7:12]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

ggarrange(p_coe1, p_coe2, p_coe3, p_coe4, ncol = 2, nrow = 2)
```

Empezando por el primer coeficiente podemos observar que el género masculino tiene los valores más altos y distribuidos. Para los coeficientes que van del segundo al sexto, podemos ver que los valores femeninos van ganando relevancia según aumentamos el coeficiente, es decir, según captamos mayores frecuencias. Finalmente para los últimos coeficientes vemos que pasa evoluciona hacia lo contrario que en el primer coeficiente. Los valores para los hombres son claramente inferiores y menos distribuidos.

## Frecuencia fundamental

```{r}
# FO
# Inicializar el vector para la característica FO
FO_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  audio_de <- audio@right
  
  # Calcular FO para cada canal
  F0_iz <- FO(audio_iz, audio@samp.rate)
  F0_de <- FO(audio_de, audio@samp.rate)
  
  # Promediar los valores calculados
  FO_carac[id] <- mean(c(F0_iz, F0_de))
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica FO al marco de datos
df_caract <- cbind(df_caract, FO_carac)
```

```{r warning=FALSE}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))

# Crear el gráfico combinado
ggplot(df, aes(x = locutor, y = FO_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Frecuencia Fundamental (F0) por Locutor y Género",
    x = "Locutor",
    y = "Frecuencia Fundamental (F0)",
    color = "Género"
  ) +
  scale_y_continuous(limits = c(75, 300)) +  # Ajustar el rango del eje Y
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

```

```{r}
ggplot(df, aes(x = FO_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de F0 por Género",
    x = "Frecuencia Fundamental (F0)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```
Nuestro caso se muestra paradójico con respecto a esta característica, ya que, aunque las mujeres presentan frecuencias fundamentales altas y similares, los hombres se distribuyen en un rango más amplio, desde frecuencias bajas hasta las más altas. Este comportamiento no es el esperable teóricamente, pues se ha demostrado que las mujeres tienen una frecuencia fundamental superior a la de los hombres, lo que en nuestro caso no ocurre.

Esta discrepancia podría explicarse por factores propios de los datos, como diferencias en las condiciones de grabación o la inclusión de locutores con características atípicas, como hombres con frecuencias fundamentales altas o mujeres con frecuencias bajas. 

## Formantes

Los formantes se encuentran caracterizados por aquellas intensidades en el espectro de sonido que destacan en un señal. Altamente relacionado con el tracto vocal de la persona, produciendo una gran cantidad de formantes. Siendo los más relevantes los primeros. En concreto los dos primeros relacionados con el sonido de las vocales. A partir del segundo formante, las frecuencias le dan color a nuestra voz, hasta el punto de tener frecuencias no distinguibles para el oído humano.

Con el objetivo de una sencilla visualización, a continuación obtendremos los formantes a partir de los sonidos de las vocales "a" y "e". Tras haber recortado algunos de los audios grabados con el objetivo de investigar la característica.

De los formantes obtenidos hemos realizado la media por género. Por lo general, esperamos valores más altos para las mujeres. Cabe destacar que al haber recortado manualmente los audios, los fonemas no son puros. Además el tamaño de nuestra muestra es bastante reducido. Es por eso que no esperamos obtener resultados perfectos, en caso de querer comparar con otros estudios, pero sí pudiendo diferenciar. 

```{r extraccion_caract_for_1}
# Vocal E
carga <- carga_audios("../Data/vocal_e")
vocales <- carga[[1]]
gen <- c(rep("Femenino", 5), rep("Masculino", 5))
f1_e <- numeric(length(vocales))
f2_e <- numeric(length(vocales))
id <- 1

for (vocal in vocales) {
  f <- findformants(vocal@left, fs=vocal@samp.rate, verify=FALSE)
  fs <- f$formant[f$formant < 2500]
  f1_e[id] <- fs[1]
  f2_e[id] <- fs[2]
  id <- id + 1
}

df_e <- data.frame(genero = gen,
                   f1 = f1_e,
                   f2 = f2_e)
df_e <- df_e %>%
  group_by(genero) %>%
  summarise(f1 = round(mean(f1, na.rm=TRUE), 2),
            f2 = round(mean(f2, na.rm=TRUE), 2))

# Vocal A
carga <- carga_audios("../Data/vocal_a")
vocales <- carga[[1]]
f1_a <- numeric(length(vocales))
f2_a <- numeric(length(vocales))
id <- 1
for (vocal in vocales) {
  f <- findformants(vocal@left, fs=vocal@samp.rate, verify=FALSE)
  fs <- f$formant[f$formant < 2000]
  f1_a[id] <- fs[1]
  f2_a[id] <- fs[2]
  id <- id + 1
}

df_a <- data.frame(genero = gen,
                   f1 = f1_a,
                   f2 = f2_a)
df_a <- df_a %>%
  group_by(genero) %>%
  summarise(f1 = mean(f1, na.rm=TRUE),
            f2 = mean(f2, na.rm=TRUE))
```

```{r analisis_for_1}
colnames(df_e) <- c("Genero", "F1 (vocal E)", "F2 (vocal E)")
colnames(df_a) <- c("Genero", "F1 (vocal A)", "F2 (vocal A)")

knitr::kable(cbind(df_a, df_e[-1]), format = "latex", 
             booktabs = TRUE, 
             caption = "Formantes en las vocales A y E", 
             align = 'lll', centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)
```

A partir de los datos obtenidos, podemos diferenciar claramente el género del locutor. Para el caso de la vocal E, los formantes 1 y 2, para las mujeres son más altos. Sin embargo, si luego observamos en el caso de la vocal A, para el formante 1, tenemos valores muy similares, es solo en el formante 2 donde podemos diferenciar. Teniendo otra vez valores inferiores en los hombres.

Debido al coste computacional implicado en el cálculo de formantes, hemos decidido precalcularlo utilizando la aplicación PRAAT. De cada audio extraemos una palabra con ciertos fonemas caraterísticos. Teniendo en cuenta la complejidad que buscamos con nuestro clasificador, bajo el objetivo de clasificar el género del locutor, resumiremos los datos. En vez de utilizar los formantes en su total extensión en el tiempo calcularemos estadísticos, como la media y la desviación estándar para los 3 primeros formantes.

```{r extraccion_caract_for_2}
df_for <- carga_formants("../Data/formants_as.txt", audios)
df_caract <- cbind(df_caract, df_for[-1])
```

Si analizamos los valores de los formantes visualizamos en parte lo esperado. Para el primer formante tenemos frecuencias más altas en las mujeres. Según pasamos a los siguientes a los siguientes formantes notamos luna evolución donde las frecuencias en los hombres van tomando más protagonismo.

```{r analisis_for_2}
df_analisis_for <- df_caract %>%
  left_join(meta_audios[c("genero", "pista", "id_audio")], by = "id_audio") %>%
  dplyr::filter(pista %in% paste0("0", c(1:5)))

p_for1 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = factor(pista), y = f1_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F1") +
            xlab("Pista audio") + ylab("Valor frecuencia")

p_for2 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = factor(pista), y = f2_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F2") +
            xlab("Pista audio") + ylab("Valor frecuencia")

p_for3 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = factor(pista), y = f3_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F3") +
            xlab("Pista audio") + ylab("Valor frecuencia")

ggarrange(p_for1,
          ggarrange(p_for2, p_for3, ncol = 2),
          nrow = 2)
```

## Zero Crossing Rate

```{r}
# ZCR
# Inicializar el vector para la característica ZCR
ZCR_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  audio_de <- audio@right
  
  # Calcular FO para cada canal
  ZCR_iz <- ZCR_FUN(audio_iz, audio@samp.rate)
  ZCR_de <-ZCR_FUN(audio_de, audio@samp.rate)
  
  # Promediar los valores calculados
  ZCR_carac[id] <- mean(c(ZCR_iz, ZCR_de))
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica ZCR al marco de datos
df_caract <- cbind(df_caract, ZCR_carac)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))

# Crear el gráfico combinado
ggplot(df, aes(x = locutor, y = ZCR_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Zero Crossing rate (ZCR) por Locutor y Género",
    x = "Locutor",
    y = "Zero Crossing rate (ZCR)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

```{r}
ggplot(df, aes(x = ZCR_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de ZCR por Género",
    x = "Zero Crossing Rate (ZCR)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

En esta característica se observa un comportamiento más acorde con lo esperado teóricamente. El Zero Crossing Rate (ZCR) tiende a ser mayor en las mujeres que en los hombres, lo cual es consistente con las propiedades de las señales de voz. Este patrón se aprecia tanto en la distribución general del ZCR por género como en los locutores individuales, donde, por norma general, las mujeres presentan valores de ZCR superiores. Este resultado refleja que el ZCR es una característica adecuada para diferenciar géneros en nuestro caso de estudio.

## Linear Predictive Coding

El Linear Predictive Coding (LPC) es un modelo matemático que describe una señal de audio en función de una combinación lineal de sus valores pasados. En lugar de representar la señal de audio directamente, se modela como una secuencia de coeficientes que predicen el valor futuro de la señal basándose en los valores previos. Los coeficientes LPC son los parámetros que caracterizan este modelo de predicción lineal. En el contexto de la LPCC, los coeficientes son derivados a partir de los coeficientes LPC mediante una transformación matemática, y son usados frecuentemente como una representación compacta y eficiente de la información espectral de la señal. Nuestro objetivo es analizar si los coeficientes obtenidos para los audios con locutores hombres tienen patrones distintos a los obtenidos para las locutoras.

Para obtener los coeficientes LPCC seguiremos los siguientes pasos:

1. Preprocesamiento de la señal: La señal de audio se segmenta en ventanas de corto tiempo (frames) para su análisis.

2. Cálculo de los coeficientes LPC: A partir de cada segmento de la señal, se calcula un conjunto de coeficientes LPC (usaremos la función lpc). Estos coeficientes describen cómo la señal futura puede ser predicha a partir de los valores anteriores.

3. Transformación a LPCC: Una vez obtenidos los coeficientes LPC, se aplican transformaciones matemáticas (como la transformada cepstral) para obtener los coeficientes LPCC. 

Usaremos orden 4, lo que significa que obtendremos 4 coeficientes por ventana. Por cada audio calcularemos la media y desviación estándar de los coeficientes.

```{r}
# Parámetros
frame_size <- 512  # Tamaño del frame
overlap <- 128     # Solapamiento
order <- 4         # Orden de LPC

# Aplicar la función a todos los audios y calcular los LPCCs
lpcc_results <- lapply(audios, function(wav) {
  # Convertir audio a vector (usando el canal 'left' para señales mono)
  signal <- as.vector(wav@left)
  calc_lpcc(signal, frame_size, overlap, order)
})
```

```{r}
df_list <- lapply(1:length(lpcc_results), function(i) {
  # Para cada audio, expandimos cada ventana y sus coeficientes LPCC
  audio_lpcc <- lpcc_results[[i]]
  # Convertir cada ventana (lista de 4 coeficientes) en un dataframe de 4 columnas
  audio_df <- do.call(rbind, lapply(audio_lpcc, function(window) {
    data.frame(lpcc_1 = window[1], lpcc_2 = window[2], lpcc_3 = window[3], lpcc_4 = window[4])
  }))
  
  # Agregar una columna 'audio_id' para identificar el audio
  audio_df$id_audio <- i
  return(audio_df)
})

# Combinar todas las listas de dataframes en uno solo
final_df <- do.call(rbind, df_list)

df_totales <- final_df %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df_totales$locutor <- factor(df_totales$locutor,
                             levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
```

```{r, warning=FALSE}
df <- df_totales %>%
group_by(id_audio, genero, pista, locutor) %>%
summarise(
  lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
  lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
  lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
  lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  .groups="keep"
)

df_caract <- cbind(df_caract, df[-c(1:6)])
```

A continuación, analizaremos si es posible identificar patrones en los coeficientes que permitan discernir el género del locutor.

Vamos a filtrar por la pista de audio y veremos si se aprecian diferencias entre la media y la desviación estándar en función del género del locutor. Hemos ignorado el valor del primer coeficiente pues siempre es 1.

```{r}
# Resumen descriptivo de los coeficientes LPCC por género
# Calcular estadísticas descriptivas por género
summary_stats <- df_totales %>%
  dplyr::filter(pista == '02') %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  )

names(summary_stats) <- c("genero", paste0(rep(paste0("C", 1:4), each=2), c("_M", "_SD")))

# Suponiendo que tu dataframe se llama df_coefs
knitr::kable(summary_stats[-c(2, 3)], format = "latex", 
             booktabs = TRUE, 
             caption = "Media y desviación del LPCC para la pista 2", 
             align = 'ccccccccc',  # Ajusta según el número de columnas
             centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)

# Resumen descriptivo de los coeficientes LPCC por género
# Calcular estadísticas descriptivas por género
summary_stats <- df_totales %>%
  dplyr::filter(pista == '03') %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  )

names(summary_stats) <- c("genero", paste0(rep(paste0("C", 1:4), each=2), c("_M", "_SD")))

# Suponiendo que tu dataframe se llama df_coefs
knitr::kable(summary_stats[-c(2, 3)], format = "latex", 
             booktabs = TRUE, 
             caption = "Media y desviación del LPCC para la pista 3", 
             align = 'ccccccccc',  # Ajusta según el número de columnas
             centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)
```

Notamos que ciertos patrones se repiten para todas las pistas de audio. La media de los coeficientes 2 y 4 correspondientes a los chicos es mayor que la de las chicas en todas las pistas. Mientras que con el coeficiente 3 pasa al contrario. También podemos destacar que la desviación estándar de los coeficientes es en todos los casos superior para los locutores chicos que para las chicas. 

## Short-Time Energy

La característica Short-Time Energy (STE) mide la energía contenida en una señal dentro de pequeñas ventanas temporales. Puede ser muy útil para analizar variaciones de energía a lo largo del tiempo. Para nuestro caso, puede ser particularmente interesante porque las variaciones de energía a lo largo del tiempo pueden reflejar características importantes de la voz humana, como la intensidad, los patrones de habla y las pausas.

El procedimiento que hemos seguido consiste en tomar cada señal (audio) y dividirla en ventanas temporales. Luego, calculamos la energía cuadrática media para cada ventana y la normalizamos. Finalmente, promediamos los valores de STE normalizados obtenidos en cada audio para obtener un único valor representativo.

```{r extraccion_caract_ste}
ste_c <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  ste_c[id] <- mean(STE(audio, audio@samp.rate * 0.02, 50)[,2])
  id = id + 1
}

# Agregar caracteristica
df_caract <- cbind(df_caract, ste_c)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))

```

```{r}
ggplot(df, aes(x = locutor, y = ste_c, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3) +  # Boxplots por género
  labs(
    title = "Short-Time Energy (STE) por Locutor y Género",
    x = "Locutor",
    y = "Short-Time Energy (STE)",
    color = "Género"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Observamos que las locutoras tienen una mayor variabilidad en la STE en comparación con los locutores hombres. Esto podría deberse a características propias de las voces femeninas, como el rango de modulación, que es más amplio. Además, las locutoras presentan valores de STE más altos en general. Esto se refleja en una mediana de aproximadamente 0.10, mientras que en los locutores hombres la mediana se encuentra alrededor de 0.07. Esto puede tener relación con la frecuencia fundamental. Las voces femeninas suelen tener una frecuencia fundamental más alta que las voces masculinas. Dado que la STE mide la energía de la señal acústica, una frecuencia fundamental más alta puede generar más energía en las frecuencias superiores, lo que puede reflejarse en un valor más alto de STE.


```{r}
ggplot(df, aes(x = ste_c, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de STE por Género",
    x = "Short-Time Energy (STE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

Las curvas de densidad muestran que las voces femeninas tienen una distribución más amplia y desplazada hacia valores más altos de STE, lo que confirma lo que hemos visto en el boxplot. Los locutores hombres, por otro lado, presentan una distribución más concentrada y con menor dispersión. Aunque hay diferencias entre géneros, también se observa cierto solapamiento en las densidades de STE. Esto indica que, si bien hay tendencias generales, la STE por sí sola puede no ser completamente discriminativa entre géneros en todos los casos.

En conclusión, los resultados sugieren que la Short-Time Energy es una métrica que capta diferencias significativas entre géneros, aunque también está influenciada por características individuales.

## Piecewise Gaussian Modeling

```{r extraccion_caract_PGM, message=FALSE, warning=FALSE}
# PGM
# Inicializar listas para almacenar los resultados
mean_charac <- numeric(length(audios))
var_charac <- numeric(length(audios))

# ID para iterar sobre los audios
id <- 1
for (audio in audios_norm) {
  PGM_left <- PGM(audio, coef=40) 
  
  # Almacenar los resultados en las listas
  mean_charac[id] <- mean(PGM_left$medias)
  var_charac[id] <- mean(PGM_left$varianzas)
  
  # Incrementar el contador
  id <- id + 1
}

# Crear un data frame con los resultados
PGM_caract <- data.frame(
  media_carac = mean_charac,
  var_carac = var_charac
)

# Agregar la característica PGM al marco de datos
df_caract <- cbind(df_caract, PGM_caract)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
```

```{r}
ggplot(df, aes(x = media_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución asociada a vectores de medias por Género",
    x = "Medias normalizadas",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```

```{r}
ggplot(df, aes(x = var_carac , fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución asociada a vectores de varianzas por Género",
    x = "Varianzas normalizadas",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```

```{r}
# Crear la nube de puntos
ggplot(df, aes(x = media_carac, y = var_charac, color = genero, shape = genero)) +
  geom_point(size = 3) +
  labs(
    x = "Medias normalizadas",
    y = "Varianzas normalizadas",
    title = "Nube de Puntos",
    color = "Género",
    shape = "Género"
  ) +
  theme_minimal()
```

## Power Spectrum

```{r extraccion_caract_4}
# PS
# Inicializar listas para almacenar los resultados
max_potencias <- numeric(length(audios))
freq_max_potencias <- numeric(length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios_norm) {
  # Acceso directo a los canales izquierdo y derecho
  audio_left <- audio@left

  # Calcular PS para canal izquierdo
  PS_left <- PS(audio_left, audio@samp.rate, span=9) #Se puede modificar el span
  
  # Almacenar los resultados en las listas
  max_potencias[id] <- max(PS_left$Potencia)
  freq_max_potencias[id] <- PS_left$Frecuencia[which.max(PS_left$Potencia)]
  
  # Incrementar el contador
  id <- id + 1
}

# Crear un data frame con los resultados
PS_caract <- data.frame(
  MaxPotencia = max_potencias,
  FrecuenciaMaxPotencia = freq_max_potencias
)

# Agregar la característica PS al marco de datos
df_caract <- cbind(df_caract, PS_caract)
```

```{r}
df <- df_caract %>%
  left_join(df, by = "id_audio") 
```

```{r}
ggplot(df, aes(x = FrecuenciaMaxPotencia, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de frecuencia asociada a potencia máxima por Género",
    x = "Frecuencia máxima",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

```{r}
ggplot(df, aes(x = MaxPotencia , fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de potencia máxima por Género",
    x = "Potencia máxima",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```

```{r}
# Crear la nube de puntos
ggplot(df, aes(x = FrecuenciaMaxPotencia, y = MaxPotencia, color = genero, shape = genero)) +
  geom_point(size = 3) +
  labs(
    x = "Frecuencia asociada a máxima potencia (Hz)",
    y = "Máxima Potencia",
    title = "Nube de Puntos",
    color = "Género",
    shape = "Género"
  ) +
  theme_minimal()
```

```{r guardar_caracteristicas}
save(df_caract, file = "../Data/caracteristicas_precalculadas.RData")
```

# Algoritmos de ML

## Clasificación

```{r algoritmos_clasificacion}
# WIP
```



