---
title: Proyecto de Clasificación de Voces de Personas
author:
  - name: Luis Albacete Caballero
    email: alcaluis@alumni.uv.es
    affiliation: UV
  - name: Julio García Bustos
    email: jugarbus@alumni.uv.es
    affiliation: UV
  - name: Gabriel Ivars Asensio
    email: gaia2@alumni.uv.es
    affiliation: UV
  - name: Noé López García
    email: nologar@alumni.uv.es
    affiliation: UV
  - name: José Miguel Palazón Caballero
    email: jomipaca@alumni.uv.es
    affiliation: UV
  - name: Joan Pedro Bruixola
    email: jopebrui@alumni.uv.es
    affiliation: UV
address:
  - code: UV
    organization: Universitat de València
    addressline: Avinguda de l'Universitat
    city: Burjassot
    state: Valencia
    postcode: 46100
    country: España
abstract: |
  La clasificación de voces de personas es un tema latente en el análisis de señales. Requiriendo extraer particularidades y características de la voz como señal. A lo largo de este trabajo obtendremos distintas características utilizando librerias o implementando nosotros la propia extracción de la característica. Bajo la finalidad de poder clasificar audios dependiendo del género del locutor.
keywords: 
  - Voces
  - Características
  - Características de la voz
  - Clasificación
  - Aprendizaje Máquina
  - Análisis de Señales
  - Identificación voces
date: "`r Sys.Date()`"
linenumbers: false
numbersections: true
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
---

# Captación de audio

A fin de poder extraer las características de los audios, requerimos en primera instancia de los audios en sí. Para ello hemos captado 70 audios, grabados por 10 compañeros del Máster de Ciencia de Datos.

Para agilizar las labores de preparación del contenido, los audios fueron grabados bajo las mismas condiciones, desde software y hardware de captación de audio como la sala de grabación e instrucciones para la grabación.

Aprovechar para agredecer a todos los colaboradores su intervención.

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(#warning=FALSE,
                      out.width = "80%",
                      fig.align = "center",
                      echo = FALSE)

# Cargar librerías
librerias <- c("tuneR",      # Lectura de archivos WAV
               "seewave",    # Manipulación mediante wavelets
               "plotly",     # Gráficos 3D
               "phonTools",  # Libreria para obtención de Formants
               "dplyr",      # Manipulacion de datos
               "kableExtra", # Kable para ajutar las tablas
               "wavelets",   # Libreria para manipulación con Wavelets
               "signal",     # Funciones de procesado de señales
               "ggplot2",    # Librería de gráficos
               "ggpubr",     # Mostrar múltiples ggplots juntos
               "lomb"
              )
pacman::p_load(char = librerias, warn.conflicts=FALSE)

# Cargar utilidades
source("../Utils/extraccion_caracteristicas.R")
source("../Utils/carga_datos.R")
```

```{r limpieza_audios}
# Cargar datos + Metadatos
carga <- carga_audios("../Data/Audios/")
audios <- carga[[1]]
meta_audios <- carga[[2]]

# 1. Eliminación ruido
audios_lim <- limpieza_senales(audios)

# 2. Normalización
audios <- normalizacion(audios_lim, norm_amplitud=FALSE)
audios_norm <- normalizacion(audios_lim)

# 3. DataFrame características
df_caract <- data.frame("id_audio" = meta_audios$id_audio)
```

# Característica de la voz

En el desarrollo de este apartado probaremos distintas técnicas de extracción de características.

## Mel Frecuency Cepstral Coefficients (MFCC)

Esta característica es una de las más comunes para reconocimiento de voces. Combina el análisis de cepstrums con una escala perceptual de las frecuencias.

Los coeficientes de Mel se basan en la percepción humana utilizando una escala linealmente separada para los valores de la señal inferior a 1KHz y una logarítmica en caso de ser superior.

El cálculo de los coeficientes se constituye de los siguientes pasos:

1. **Pre-emphasis**. El primer paso consiste en pasar la señal por un filtro paso alto. En este se incrementará la energía de las altas frecuencias. Utilizando la siguiente fórmula: $y(n) = x(n) - a * x(n - 1)$ con valores de $a$ entre 0.9 y 1.
2. **Frame blocking**. Consiste en la división de la señal en frames de entre 20 y 30 ms.
3. **Windowing**. Seguidamente se aplica un efecto de "windowing" de manera que los bordes de la señal (frame) sean suavizados (más cercano a 0 en los extremos). La función de ventana que se utiliza es la de Hamming.
4. **Discrete Fourier Transform**. Como deseamos recoger las energías en el dominio de las frecuencias debemos hacer un cambio de dominio mediante la DFT.
5. **Mel-Filter**. Se suma las energías de las componentes espectrales por cada escala de Mel. Al resultado del filtrado se escalará logarítmicamente.
5. **Discrete Cousine Transform**. Finalmente se aplicará la siguiente transformada: $C(n) = \sum Ek * cos(n * (k-0.5) * \pi / 40)$ para volver al dominio del tiempo. Obteniendo los coeficientes de Mel que recogen la energía total escalada por cada banda de frecuencia por cada intervalo.

```{r extraccion_caract_mfcc, warning=FALSE}
mel_coefs <- list()

id <- 1
for (audio in audios_norm) {
  # Calculo de coeficientes y media por cada audio
  coefs <- colMeans(melfcc(audio,
                           minfreq = 50,
                           maxfreq = 4000,
                           sr = audio@samp.rate,
                           wintime = 0.025, usecmp = TRUE))
  
  # De los 12 coeficientes haremos agrupaciones:
  mel_coefs[[id]] <- c(coefs[1], mean(coefs[2:3]),
                       mean(coefs[4:6]), mean(coefs[7:12]))
  id <- id + 1
}

mel_mat <- matrix(unlist(mel_coefs), byrow=TRUE, ncol=4)
colnames(mel_mat) <- c("MEL_COE1", "MEL_COE2_3", "MEL_COE4_6",
                       "MEL_COE7_12")

df_caract <- cbind(df_caract, mel_mat)
```

Agruparemos los coeficientes para un análisis más conciso basandonos en el concepto del oído humano. Escalando poco a poco las agrupaciones. En la siguiente gráfica veremos los coeficientes entre hombres y mujeres para las 5 primeras pistas de audio.

```{r analisis_mfcc}
df_analisis_mel <- df_caract %>%
  left_join(meta_audios[c("genero", "pista", "id_audio")], by = "id_audio") %>%
  dplyr::filter(pista %in% paste0("0", c(1:5)))

p_coe1 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE1, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [1]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe2 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE2_3, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [2:3]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe3 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE4_6, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [4:6]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe4 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = factor(pista), y = MEL_COE7_12, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [7:12]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

ggarrange(p_coe1, p_coe2, p_coe3, p_coe4, ncol = 2, nrow = 2)
```

Empezando por el primer coeficiente podemos observar que el género masculino tiene los valores más altos y distribuidos. Para los coeficientes que van del segundo al sexto, podemos ver que los valores femeninos van ganando relevancia según aumentamos el coeficiente, es decir, según captamos mayores frecuencias. Finalmente para los últimos coeficientes vemos que pasa evoluciona hacia lo contrario que en el primer coeficiente. Los valores para los hombres son claramente inferiores y menos distribuidos.

## Frecuencia fundamental

```{r}
# FO
# Inicializar el vector para la característica FO
FO_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  audio_de <- audio@right
  
  # Calcular FO para cada canal
  F0_iz <- FO(audio_iz, audio@samp.rate)
  F0_de <- FO(audio_de, audio@samp.rate)
  
  # Promediar los valores calculados
  FO_carac[id] <- mean(c(F0_iz, F0_de))
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica FO al marco de datos
df_caract <- cbind(df_caract, FO_carac)
```

```{r warning=FALSE}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))

# Crear el gráfico combinado
ggplot(df, aes(x = locutor, y = FO_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Frecuencia Fundamental (F0) por Locutor y Género",
    x = "Locutor",
    y = "Frecuencia Fundamental (F0)",
    color = "Género"
  ) +
  scale_y_continuous(limits = c(75, 300)) +  # Ajustar el rango del eje Y
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

```

```{r}
ggplot(df, aes(x = FO_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de F0 por Género",
    x = "Frecuencia Fundamental (F0)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```
Nuestro caso se muestra paradójico con respecto a esta característica, ya que, aunque las mujeres presentan frecuencias fundamentales altas y similares, los hombres se distribuyen en un rango más amplio, desde frecuencias bajas hasta las más altas. Este comportamiento no es el esperable teóricamente, pues se ha demostrado que las mujeres tienen una frecuencia fundamental superior a la de los hombres, lo que en nuestro caso no ocurre.

Esta discrepancia podría explicarse por factores propios de los datos, como diferencias en las condiciones de grabación o la inclusión de locutores con características atípicas, como hombres con frecuencias fundamentales altas o mujeres con frecuencias bajas. 

## Formantes

Los formantes se encuentran caracterizados por aquellas intensidades en el espectro de sonido que destacan en un señal. Altamente relacionado con el tracto vocal de la persona, produciendo una gran cantidad de formantes. Siendo los más relevantes los primeros. En concreto los dos primeros relacionados con el sonido de las vocales. A partir del segundo formante, las frecuencias le dan color a nuestra voz, hasta el punto de tener frecuencias no distinguibles para el oído humano.

Con el objetivo de una sencilla visualización, a continuación obtendremos los formantes a partir de los sonidos de las vocales "a" y "e". Tras haber recortado algunos de los audios grabados con el objetivo de investigar la característica.

De los formantes obtenidos hemos realizado la media por género. Por lo general, esperamos valores más altos para las mujeres. Cabe destacar que al haber recortado manualmente los audios, los fonemas no son puros. Además el tamaño de nuestra muestra es bastante reducido. Es por eso que no esperamos obtener resultados perfectos, en caso de querer comparar con otros estudios, pero sí pudiendo diferenciar. 

```{r extraccion_caract_for_1}
# Vocal E
carga <- carga_audios("../Data/vocal_e")
vocales <- carga[[1]]
gen <- c(rep("Femenino", 5), rep("Masculino", 5))
f1_e <- numeric(length(vocales))
f2_e <- numeric(length(vocales))
id <- 1

for (vocal in vocales) {
  f <- findformants(vocal@left, fs=vocal@samp.rate, verify=FALSE)
  fs <- f$formant[f$formant < 2500]
  f1_e[id] <- fs[1]
  f2_e[id] <- fs[2]
  id <- id + 1
}

df_e <- data.frame(genero = gen,
                   f1 = f1_e,
                   f2 = f2_e)
df_e <- df_e %>%
  group_by(genero) %>%
  summarise(f1 = round(mean(f1, na.rm=TRUE), 2),
            f2 = round(mean(f2, na.rm=TRUE), 2))

# Vocal A
carga <- carga_audios("../Data/vocal_a")
vocales <- carga[[1]]
f1_a <- numeric(length(vocales))
f2_a <- numeric(length(vocales))
id <- 1
for (vocal in vocales) {
  f <- findformants(vocal@left, fs=vocal@samp.rate, verify=FALSE)
  fs <- f$formant[f$formant < 2000]
  f1_a[id] <- fs[1]
  f2_a[id] <- fs[2]
  id <- id + 1
}

df_a <- data.frame(genero = gen,
                   f1 = f1_a,
                   f2 = f2_a)
df_a <- df_a %>%
  group_by(genero) %>%
  summarise(f1 = mean(f1, na.rm=TRUE),
            f2 = mean(f2, na.rm=TRUE))
```

```{r analisis_for_1}
colnames(df_e) <- c("Genero", "F1 (vocal E)", "F2 (vocal E)")
colnames(df_a) <- c("Genero", "F1 (vocal A)", "F2 (vocal A)")

knitr::kable(cbind(df_a, df_e[-1]), format = "latex", 
             booktabs = TRUE, 
             caption = "Formantes en las vocales A y E", 
             align = 'lll', centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)
```

A partir de los datos obtenidos, podemos diferenciar claramente el género del locutor. Para el caso de la vocal E, los formantes 1 y 2, para las mujeres son más altos. Sin embargo, si luego observamos en el caso de la vocal A, para el formante 1, tenemos valores muy similares, es solo en el formante 2 donde podemos diferenciar. Teniendo otra vez valores inferiores en los hombres.

Debido al coste computacional implicado en el cálculo de formantes, hemos decidido precalcularlo utilizando la aplicación PRAAT. De cada audio extraemos una palabra con ciertos fonemas caraterísticos. Teniendo en cuenta la complejidad que buscamos con nuestro clasificador, bajo el objetivo de clasificar el género del locutor, resumiremos los datos. En vez de utilizar los formantes en su total extensión en el tiempo calcularemos estadísticos, como la media y la desviación estándar para los 3 primeros formantes.

```{r extraccion_caract_for_2}
df_for <- carga_formants("../Data/formants_as.txt", audios)
df_caract <- cbind(df_caract, df_for[-1])
```

Si analizamos los valores de los formantes visualizamos en parte lo esperado. Para el primer formante tenemos frecuencias más altas en las mujeres. Según pasamos a los siguientes a los siguientes formantes notamos luna evolución donde las frecuencias en los hombres van tomando más protagonismo.

```{r analisis_for_2}
df_analisis_for <- df_caract %>%
  left_join(meta_audios[c("genero", "pista", "id_audio")], by = "id_audio") %>%
  dplyr::filter(pista %in% paste0("0", c(1:5)))

p_for1 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = factor(pista), y = f1_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F1") +
            xlab("Pista audio") + ylab("Valor frecuencia")

p_for2 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = factor(pista), y = f2_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F2") +
            xlab("Pista audio") + ylab("Valor frecuencia")

p_for3 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = factor(pista), y = f3_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F3") +
            xlab("Pista audio") + ylab("Valor frecuencia")

ggarrange(p_for1,
          ggarrange(p_for2, p_for3, ncol = 2),
          nrow = 2)
```

## Zero Crossing Rate

```{r}
# ZCR
# Inicializar el vector para la característica ZCR
ZCR_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  audio_de <- audio@right
  
  # Calcular FO para cada canal
  ZCR_iz <- ZCR_FUN(audio_iz, audio@samp.rate)
  ZCR_de <-ZCR_FUN(audio_de, audio@samp.rate)
  
  # Promediar los valores calculados
  ZCR_carac[id] <- mean(c(ZCR_iz, ZCR_de))
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica ZCR al marco de datos
df_caract <- cbind(df_caract, ZCR_carac)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))

# Crear el gráfico combinado
ggplot(df, aes(x = locutor, y = ZCR_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Zero Crossing rate (ZCR) por Locutor y Género",
    x = "Locutor",
    y = "Zero Crossing rate (ZCR)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

```{r}
ggplot(df, aes(x = ZCR_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de ZCR por Género",
    x = "Zero Crossing Rate (ZCR)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

En esta característica se observa un comportamiento más acorde con lo esperado teóricamente. El Zero Crossing Rate (ZCR) tiende a ser mayor en las mujeres que en los hombres, lo cual es consistente con las propiedades de las señales de voz. Este patrón se aprecia tanto en la distribución general del ZCR por género como en los locutores individuales, donde, por norma general, las mujeres presentan valores de ZCR superiores. Este resultado refleja que el ZCR es una característica adecuada para diferenciar géneros en nuestro caso de estudio.

## Linear Predictive Coding

El Linear Predictive Coding (LPC) es un modelo matemático que describe una señal de audio en función de una combinación lineal de sus valores pasados. En lugar de representar la señal de audio directamente, se modela como una secuencia de coeficientes que predicen el valor futuro de la señal basándose en los valores previos. Los coeficientes LPC son los parámetros que caracterizan este modelo de predicción lineal. En el contexto de la LPCC, los coeficientes son derivados a partir de los coeficientes LPC mediante una transformación matemática, y son usados frecuentemente como una representación compacta y eficiente de la información espectral de la señal. Nuestro objetivo es analizar si los coeficientes obtenidos para los audios con locutores hombres tienen patrones distintos a los obtenidos para las locutoras.

Para obtener los coeficientes LPCC seguiremos los siguientes pasos:

1. Preprocesamiento de la señal: La señal de audio se segmenta en ventanas de corto tiempo (frames) para su análisis.

2. Cálculo de los coeficientes LPC: A partir de cada segmento de la señal, se calcula un conjunto de coeficientes LPC (usaremos la función lpc). Estos coeficientes describen cómo la señal futura puede ser predicha a partir de los valores anteriores.

3. Transformación a LPCC: Una vez obtenidos los coeficientes LPC, se aplican transformaciones matemáticas (como la transformada cepstral) para obtener los coeficientes LPCC. 

Usaremos orden 4, lo que significa que obtendremos 4 coeficientes por ventana. Por cada audio calcularemos la media y desviación estándar de los coeficientes.

```{r}
# Parámetros
frame_size <- 512  # Tamaño del frame
overlap <- 128     # Solapamiento
order <- 4         # Orden de LPC

# Aplicar la función a todos los audios y calcular los LPCCs
lpcc_results <- lapply(audios, function(wav) {
  # Convertir audio a vector (usando el canal 'left' para señales mono)
  signal <- as.vector(wav@left)
  calc_lpcc(signal, frame_size, overlap, order)
})
```

```{r}
df_list <- lapply(1:length(lpcc_results), function(i) {
  # Para cada audio, expandimos cada ventana y sus coeficientes LPCC
  audio_lpcc <- lpcc_results[[i]]
  # Convertir cada ventana (lista de 4 coeficientes) en un dataframe de 4 columnas
  audio_df <- do.call(rbind, lapply(audio_lpcc, function(window) {
    data.frame(lpcc_1 = window[1], lpcc_2 = window[2], lpcc_3 = window[3], lpcc_4 = window[4])
  }))
  
  # Agregar una columna 'audio_id' para identificar el audio
  audio_df$id_audio <- i
  return(audio_df)
})

# Combinar todas las listas de dataframes en uno solo
final_df <- do.call(rbind, df_list)

df_totales <- final_df %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df_totales$locutor <- factor(df_totales$locutor,
                             levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
```

```{r, warning=FALSE}
df <- df_totales %>%
group_by(id_audio, genero, pista, locutor) %>%
summarise(
  lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
  lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
  lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
  lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  .groups="keep"
)

df_caract <- cbind(df_caract, df[-c(1:6)])
```

A continuación, analizaremos si es posible identificar patrones en los coeficientes que permitan discernir el género del locutor.

Vamos a filtrar por la pista de audio y veremos si se aprecian diferencias entre la media y la desviación estándar en función del género del locutor. Hemos ignorado el valor del primer coeficiente pues siempre es 1.

```{r}
# Resumen descriptivo de los coeficientes LPCC por género
# Calcular estadísticas descriptivas por género
summary_stats <- df_totales %>%
  dplyr::filter(pista == '02') %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  )

names(summary_stats) <- c("genero", paste0(rep(paste0("C", 1:4), each=2), c("_M", "_SD")))

# Suponiendo que tu dataframe se llama df_coefs
knitr::kable(summary_stats[-c(2, 3)], format = "latex", 
             booktabs = TRUE, 
             caption = "Media y desviación del LPCC para la pista 2", 
             align = 'ccccccccc',  # Ajusta según el número de columnas
             centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)

# Resumen descriptivo de los coeficientes LPCC por género
# Calcular estadísticas descriptivas por género
summary_stats <- df_totales %>%
  dplyr::filter(pista == '03') %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  )

names(summary_stats) <- c("genero", paste0(rep(paste0("C", 1:4), each=2), c("_M", "_SD")))

# Suponiendo que tu dataframe se llama df_coefs
knitr::kable(summary_stats[-c(2, 3)], format = "latex", 
             booktabs = TRUE, 
             caption = "Media y desviación del LPCC para la pista 3", 
             align = 'ccccccccc',  # Ajusta según el número de columnas
             centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)
```

Notamos que ciertos patrones se repiten para todas las pistas de audio. La media de los coeficientes 2 y 4 correspondientes a los chicos es mayor que la de las chicas en todas las pistas. Mientras que con el coeficiente 3 pasa al contrario. También podemos destacar que la desviación estándar de los coeficientes es en todos los casos superior para los locutores chicos que para las chicas. 

## Short-Time Energy

La característica Short-Time Energy (STE) mide la energía contenida en una señal dentro de pequeñas ventanas temporales. Puede ser muy útil para analizar variaciones de energía a lo largo del tiempo. Para nuestro caso, puede ser particularmente interesante porque las variaciones de energía a lo largo del tiempo pueden reflejar características importantes de la voz humana, como la intensidad, los patrones de habla y las pausas.

El procedimiento que hemos seguido consiste en tomar cada señal (audio) y dividirla en ventanas temporales. Luego, calculamos la energía cuadrática media para cada ventana y la normalizamos. Finalmente, promediamos los valores de STE normalizados obtenidos en cada audio para obtener un único valor representativo.

```{r extraccion_caract_ste}
ste_c <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  ste_c[id] <- mean(STE(audio, audio@samp.rate * 0.02, 50)[,2])
  id = id + 1
}

# Agregar caracteristica
df_caract <- cbind(df_caract, ste_c)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))

```

```{r}
ggplot(df, aes(x = locutor, y = ste_c, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3) +  # Boxplots por género
  labs(
    title = "Short-Time Energy (STE) por Locutor y Género",
    x = "Locutor",
    y = "Short-Time Energy (STE)",
    color = "Género"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Observamos que las locutoras tienen una mayor variabilidad en la STE en comparación con los locutores hombres. Esto podría deberse a características propias de las voces femeninas, como el rango de modulación, que es más amplio. Además, las locutoras presentan valores de STE más altos en general. Esto se refleja en una mediana de aproximadamente 0.10, mientras que en los locutores hombres la mediana se encuentra alrededor de 0.07. Esto puede tener relación con la frecuencia fundamental. Las voces femeninas suelen tener una frecuencia fundamental más alta que las voces masculinas. Dado que la STE mide la energía de la señal acústica, una frecuencia fundamental más alta puede generar más energía en las frecuencias superiores, lo que puede reflejarse en un valor más alto de STE.


```{r}
ggplot(df, aes(x = ste_c, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de STE por Género",
    x = "Short-Time Energy (STE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

Las curvas de densidad muestran que las voces femeninas tienen una distribución más amplia y desplazada hacia valores más altos de STE, lo que confirma lo que hemos visto en el boxplot. Los locutores hombres, por otro lado, presentan una distribución más concentrada y con menor dispersión. Aunque hay diferencias entre géneros, también se observa cierto solapamiento en las densidades de STE. Esto indica que, si bien hay tendencias generales, la STE por sí sola puede no ser completamente discriminativa entre géneros en todos los casos.

En conclusión, los resultados sugieren que la Short-Time Energy es una métrica que capta diferencias significativas entre géneros, aunque también está influenciada por características individuales.

## Piecewise Gaussian Modeling

El PGM es un método que utiliza una estructura a largo plazo para representar las características, llamadas Ventanas de Tiempo de Integración (ITW). En cada ITW, se tiene un vector de medias y un vector de varianzas.

Para la implementación de este método, primero calculamos el MFSC (Mel Frequency Spectral Coefficient). Es similar al MFCC obtenido anteriormente pero sin la parte de DCT (Discrete Cosine Transform). Una vez calculado, se obtiene una matriz $N \times T$ donde $T$ se refiere al número de vectores espectrales contenidos en una ITW (Ventana de Tiempo de Integración) y N al número de ventanas ITW. A continuación se modela un conjunto de $T$ vectores MFSC consecutivos mediante un modelo gaussiano. Es decir, $N \times T$ de MFSC serán modelados por N gaussianas. Al calcular la matriz de covarianza, solo tomamos el índice diagonal de la matriz de covarianza para generar el vector de varianza. Finalmente, las características del PGM son la concatenación de la media y la varianza normalizadas por sus respectivos máximos y mínimos. Para obtener una característica de media y otra de varianza por audio, hemos calculado la media de cada vector.

```{r extraccion_caract_PGM, message=FALSE, warning=FALSE}
# PGM
# Inicializar listas para almacenar los resultados
mean_charac <- numeric(length(audios))
var_charac <- numeric(length(audios))

# ID para iterar sobre los audios
id <- 1
for (audio in audios_norm) {
  PGM_left <- PGM(audio, coef=172) 
  
  # Almacenar los resultados en las listas
  mean_charac[id] <- mean(PGM_left$medias)
  var_charac[id] <- mean(PGM_left$varianzas)
  
  # Incrementar el contador
  id <- id + 1
}

# Crear un data frame con los resultados
PGM_caract <- data.frame(
  pgm_mean = mean_charac,
  pgm_var = var_charac
)

# Agregar la característica PGM al marco de datos
df_caract <- cbind(df_caract, PGM_caract)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
```


```{r}
p_for1 <- ggplot(df, aes(x = pgm_mean, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución asociada a vectores de medias por Género",
    x = "Medias normalizadas",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_for2 <- ggplot(df, aes(x = pgm_var , fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución asociada a vectores de varianzas por Género",
    x = "Varianzas normalizadas",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()


ggarrange(p_for1 ,p_for2, nrow=2)
```

Las curvas de densidad muestran que para la característica de las medias normalizada, las voces masculinas tienen una menor dispersión que las femeninas y un mayor valor en promedio, pero parecen mezclarse entre sí en un mismo rango de valores. A priori no parece una buena característica para discriminar

La característica asociada a las varianzas normalizadas podría ser un mejor discriminante tampoco parece una buena característica para discrimanr. La dispersión de las voces masculinas es muy grande, asemejandose mucho a una distribución uniforme continua (prácticamente no aportan información). En cambio, la dispersión de las voces femeninas es menor y los valores se concentran mas en torno a la media.

## Power Spectrum

Esta característica nos indica qué tan fuertes están presentes diferentes frecuencias en una señal. Para estimar el Power Spectrum, se utiliza un estimador llamado periodograma. Los pasos a seguir son los siguientes:

- **Transformada Rápida de Fourier (FFT):** Se aplica la transformada para cambiar al dominio de las frecuencias.

- **Cálculo del periodograma:** Se calcula el módulo al cuadrado de la transformada de Fourier en cada frecuencia. El resultado es el periodograma, que representa la distribución de la potencia de la señal en función de la frecuencia.

- **Frecuencia de máxima potencia en el espectro:** Se identifica la frecuencia en la que se encuentra el valor máximo del espectro de potencia. Este valor de potencia máxima, junto con su frecuencia asociada, se utilizan como características clave para la clasificación de cada audio.

```{r extraccion_caract_PS}
# PS
# Inicializar listas para almacenar los resultados
max_potencias <- numeric(length(audios))
freq_max_potencias <- numeric(length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios_norm) {
  # Acceso directo a los canales izquierdo y derecho
  audio_left <- audio@left

  # Calcular PS para canal izquierdo
  PS_left <- PS(audio_left, audio@samp.rate, span=9) #Se puede modificar el span
  
  # Almacenar los resultados en las listas
  max_potencias[id] <- max(PS_left$Potencia)
  freq_max_potencias[id] <- PS_left$Frecuencia[which.max(PS_left$Potencia)]
  
  # Incrementar el contador
  id <- id + 1
}

# Crear un data frame con los resultados
PS_caract <- data.frame(
  ps_pow = max_potencias,
  ps_frec = freq_max_potencias
)

# Agregar la característica PS al marco de datos
df_caract <- cbind(df_caract, PS_caract)
```

```{r}
df <- df_caract %>%
  left_join(df, by = "id_audio") 
```

```{r}
p_for1 <- ggplot(df, aes(x = ps_frec, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de frecuencia asociada a potencia máxima por Género",
    x = "Frecuencia máxima",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_for2 <- ggplot(df, aes(x = ps_pow , fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de potencia máxima por Género",
    x = "Potencia máxima",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()


ggarrange(p_for2 ,p_for1, nrow=2)
```


Se puede observar una mayor concentración de valores más bajos de potencia máxima en las voces masculinas mientras que en las voces femeninas se muestra una distribución más dispersa, con algunos valores más altos de potencia máxima.

La distribución de la frecuencia de máxima potencia en el espectro, a priori, parece un poco mas determinante para la tarea de clasificación. En las voces femeninas hay una mayor densidad en frecuencias medias/altas (alrededor de 190 Hz) mientras que en las voces masculinas pueden diferenciarse dos grupos de frecuencias (bajas y altas), por lo que no parecen estar tan concentradas alrededor de una única frecuencia media.



```{r}
# Crear la nube de puntos
ggplot(df, aes(x = ps_frec, y = ps_pow, color = genero, shape = genero)) +
  geom_point(size = 3) +
  labs(
    x = "Frecuencia asociada a máxima potencia (Hz)",
    y = "Máxima Potencia",
    title = "Nube de Puntos",
    color = "Género",
    shape = "Género"
  ) +
  theme_minimal()
```



En la nube de puntos se puede apreciar los dos grupos de voces masculinas que se han comentado anteriormente. Las que se encuentran en el rango de bajas frecuencias se discriminan perfectamente visualmente, pero luego hay otros audios que se mezclan con los de frecuencias medias/altas de las voces femeninas. Como se puede observar, algunas voces femeninas parecen ser las que tienen mayor potencia entre todos los audios.


El outlier situado a la derecha del eje de frecuencias corresponde al audio *f_06_05* y se podría asociar a ruido, en este caso, al portazo de una puerta.


## Harmonic-to-Noise Ratio (HNR)

Esta característica mide la proporción entre la energía de la componente armónica de la señal respecto al ruido presente. Aunque su principal uso es en el área de la sanidad para detectar patologías mediante la voz, también nos puede ayudar a distinguir la claridad y ciertas particularidades de las voces.

Para ello, nos ayudaremos de la descomposición mediante wavelets para separar las Aproximaciones (componente de baja frecuencia de la señal normalmente interpretada como la parte harmónica) de los Detalles (componente de alta frecuencia relacionada con el ruido).

```{r extraccion_caract_HNR, warning=FALSE}
HNR_carac <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  # Calcular HNR para la señal y almacenarla
  HNR_carac[id] <- HNR(audio)
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica HNR al marco de datos
df_caract <- cbind(df_caract, HNR_carac)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor,
                     levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
```

```{r}
ggplot(df, aes(x = HNR_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución del Harmonic-to-Noise Ratio (HNR) por Género",
    x = "Harmonic-to-Noise Ratio (HNR)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

```{r}
ggplot(df, aes(x = locutor, y = HNR_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Harmonic-to-Noise Ratio (HNR) por Locutor y Género",
    x = "Locutor",
    y = "Harmonic-to-Noise Ratio (HNR)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

El gráfico de distribución de densidades muestra una clara diferencia de tendencia entre géneros; mientras que las voces masculinas suelen encontrarse más cerca de la media cercana a 62, el HNR de las voces femeninas está distribuido de forma más uniforme en el rango de valores. De esta forma, valores de HNR cercanos a los extremos podrían indicar mayor probabilidad de que la voz grabada sea femenina y valores cercanos a la media lo contrario. No obstante, esta diferencia de tendencias según el género no es muy evidente o conlcuyente, por lo que este indicador por si mismo puede que no sea suficiente pero potencialmente puede aportar información relevante a un algoritmo de clasificación.

Centrándonos en el HNR según locutor, podemos ver que en el caso del género femenino hay cierta constancia en los valores, agrupándose en un rango no muy amplio que podría ayudar a su identificación. En cuánto al género masculino, sus HNR se encuentran distribuidos en intervalos semejantes por lo que podría ser considerablemente difícil discernir entre locutores masculinos.

## Centroides Espectrales (CE)

Esta medida pretende caracterizar el espectro del audio señalando el "centro de masa" de su energía. Dicho centro se calcula realizando la media ponderada de las frecuencias encontradas usando una transformada de Fourier y usando sus amplitudes como pesos. 

El centroide espectral de un audio tiene profunda relación con el timbre del mismo, en particular con su "brightness"; esta característica puede ser muy útil a la hora de diferenciar entre géneros e incluso de identificar patrones únicos en cada locutor.

Para su implementación en R nos ayudaremos de la librería seewave, la cual cuenta con esta medida dentro del resumen de estadísticas presentes en la función "specprop".

```{r extraccion_caract_CE, warning=FALSE}
CE_carac <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  # Calcular CE para la señal y almacenarla
  CE_carac[id] <- CE(audio)
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica HNR al marco de datos
df_caract <- cbind(df_caract, CE_carac)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor, levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
```

```{r}
ggplot(df, aes(x = CE_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución del Centroide Espectral (CE) por Género",
    x = "Centroide Espectral (CE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()
```

```{r}
ggplot(df, aes(x = locutor, y = CE_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Centroide Espectral (CE) por Locutor y Género",
    x = "Locutor",
    y = "Centroide Espectral (CE)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

Notamos un comportamiento muy semejante al de la característica Harmonic-to-Noise Ratio (HNR): los CE de las voces masculinas se encuentran mucho más concentrados en torno a una media cercana a 175 mientras que las voces femeninas se reparten de forma más homogénea a lo largo del rango total 100-260, teniendo dos máximos locales en 120 y 230.

En cuánto al análisis por locutor, podemos observar considerable variabilidad de centroides espectrales dependiendo del audio (salvo en el caso del locutor 01), lo que puede dificultar el uso de esta medida para la identificación concreta del locutor. Sin embargo, al igual que en características anteriores la diferencia de distribución de densidad entre géneros puede aportar información relevante a la hora de clasificarlos.

```{r guardar_caracteristicas}
save(df_caract, file = "../Data/caracteristicas_precalculadas.RData")
```


# Algoritmos de ML

## Clustering

```{r algoritmos_clustering}
# WIP
```

## Clasificación

```{r algoritmos_clasificacion}
# WIP
```



