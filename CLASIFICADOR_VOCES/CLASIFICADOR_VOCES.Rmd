---
title: Proyecto de Clasificación de Voces de Personas
author:
  - name: Luis Albacete Caballero
    email: alcaluis@alumni.uv.es
    affiliation: UV
  - name: Julio García Bustos
    email: jugarbus@alumni.uv.es
    affiliation: UV
  - name: Gabriel Ivars Asensio
    email: gaia2@alumni.uv.es
    affiliation: UV
  - name: Noé López García
    email: nologar@alumni.uv.es
    affiliation: UV
  - name: José Miguel Palazón Caballero
    email: jomipaca@alumni.uv.es
    affiliation: UV
  - name: Joan Pedro Bruixola
    email: jopebrui@alumni.uv.es
    affiliation: UV
address:
  - code: UV
    organization: Universitat de València
    addressline: Avinguda de l'Universitat
    city: Burjassot
    state: Valencia
    postcode: 46100
    country: España
abstract: |
  La clasificación de voces de personas es un tema latente en el análisis de señales. Requiriendo extraer particularidades y características de la voz como señal. A lo largo de este trabajo obtendremos distintas características utilizando librerias o implementando nosotros la propia extracción de la característica. Bajo la finalidad de poder clasificar audios dependiendo del género del locutor.
keywords: 
  - Voces
  - Características
  - Características de la voz
  - Clasificación
  - Aprendizaje Máquina
  - Análisis de Señales
  - Identificación de voces
date: "`r Sys.Date()`"
linenumbers: false
numbersections: true
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
---

# Captación de audio

A fin de poder extraer las características de los audios, requerimos en primera instancia de los audios en sí. Para ello hemos captado 70 audios, grabados por 10 compañeros del Máster de Ciencia de Datos.

Para agilizar las labores de preparación del contenido, los audios fueron grabados bajo las mismas condiciones, desde software y hardware de captación de audio como la sala de grabación e instrucciones para la grabación.

Aprovechar para agredecer a todos los colaboradores su intervención.

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      out.width = "80%",
                      fig.align = "center",
                      echo = FALSE)

# Cargar librerías
librerias <- c("tuneR",        # Lectura de archivos WAV
               "seewave",      # Manipulación mediante wavelets
               "plotly",       # Gráficos 3D
               "phonTools",    # Libreria para obtención de Formants
               "dplyr",        # Manipulacion de datos
               "kableExtra",   # Kable para ajutar las tablas
               "wavelets",     # Libreria para manipulación con Wavelets
               "signal",       # Funciones de procesado de señales
               "ggplot2",      # Librería de gráficos
               "ggpubr",       # Mostrar múltiples ggplots juntos
               "lomb",         # Análisis espectral de series temporales
               "purrr",        # Programación funcional y manejo de listas
               "caret",        # Creación y evaluación de modelos predictivos
               "randomForest", # Bosques aleatorios para clasificación y regresión
               "e1071"         # SVM, clústeres y otras herramientas de machine learning
              )

pacman::p_load(char = librerias, warn.conflicts=FALSE)

# Cargar utilidades
source("../Utils/extraccion_caracteristicas.R")
source("../Utils/carga_datos.R")
```

```{r limpieza_audios}
# Cargar datos + Metadatos
carga <- carga_audios("../Data/Audios/")
audios <- carga[[1]]
meta_audios <- carga[[2]]

# 1. Eliminación ruido
audios_lim <- limpieza_senales(audios)

# 2. Normalización
audios <- normalizacion(audios_lim, norm_amplitud=FALSE)
audios_norm <- normalizacion(audios_lim)

# 3. DataFrame características
df_caract <- data.frame("id_audio" = meta_audios$id_audio)
```

# Características de la voz

En el desarrollo de este apartado probaremos distintas técnicas de extracción de características.

## Mel Frecuency Cepstral Coefficients (MFCC)

Esta característica es una de las más comunes para reconocimiento de voces. Combina el análisis de cepstrums con una escala perceptual de las frecuencias.

Los coeficientes de Mel se basan en la percepción humana utilizando una escala linealmente separada para los valores de la señal inferior a 1KHz y una logarítmica en caso de ser superior.

El cálculo de los coeficientes se constituye de los siguientes pasos:

1. **Pre-emphasis**. El primer paso consiste en pasar la señal por un filtro paso alto. En este se incrementará la energía de las altas frecuencias. Utilizando la siguiente fórmula: $y(n) = x(n) - a * x(n - 1)$ con valores de $a$ entre 0.9 y 1.
2. **Frame blocking**. Consiste en la división de la señal en frames de entre 20 y 30 ms.
3. **Windowing**. Seguidamente se aplica un efecto de "windowing" de manera que los bordes de la señal (frame) sean suavizados (más cercano a 0 en los extremos). La función de ventana que se utiliza es la de Hamming.
4. **Discrete Fourier Transform**. Como deseamos recoger las energías en el dominio de las frecuencias debemos hacer un cambio de dominio mediante la DFT.
5. **Mel-Filter**. Se suma las energías de las componentes espectrales por cada escala de Mel. Al resultado del filtrado se escalará logarítmicamente.
5. **Discrete Cousine Transform**. Finalmente se aplicará la siguiente transformada: $C(n) = \sum Ek * cos(n * (k-0.5) * \pi / 40)$ para volver al dominio del tiempo. Obteniendo los coeficientes de Mel que recogen la energía total escalada por cada banda de frecuencia por cada intervalo.

```{r extraccion_caract_mfcc, warning=FALSE}
mel_coefs <- list()

id <- 1
for (audio in audios_norm) {
  # Calculo de coeficientes y media por cada audio
  coefs <- colMeans(melfcc(audio,
                           minfreq = 50,
                           maxfreq = 4000,
                           sr = audio@samp.rate,
                           wintime = 0.025, usecmp = TRUE))
  
  # De los 12 coeficientes haremos agrupaciones:
  mel_coefs[[id]] <- c(coefs[1], mean(coefs[2:3]),
                       mean(coefs[4:6]), mean(coefs[7:12]))
  id <- id + 1
}

mel_mat <- matrix(unlist(mel_coefs), byrow=TRUE, ncol=4)
colnames(mel_mat) <- c("MEL_COE1", "MEL_COE2_3", "MEL_COE4_6",
                       "MEL_COE7_12")

df_caract <- cbind(df_caract, mel_mat)
```

Agruparemos los coeficientes para un análisis más conciso basandonos en el concepto del oído humano. Escalando poco a poco las agrupaciones. En la siguiente gráfica veremos los coeficientes entre hombres y mujeres para las 5 primeras pistas de audio.

```{r analisis_mfcc}
df_analisis_mel <- df_caract %>%
  left_join(meta_audios[c("genero", "pista", "id_audio")], by = "id_audio") %>%
  dplyr::filter(pista %in% paste0("0", c(1:5)))

p_coe1 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = pista, y = MEL_COE1, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [1]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe2 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = pista, y = MEL_COE2_3, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [2:3]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe3 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = pista, y = MEL_COE4_6, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [4:6]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

p_coe4 <- ggplot(data = df_analisis_mel,
                 mapping = aes(x = pista, y = MEL_COE7_12, fill = genero)) +
            geom_boxplot() +
            ggtitle("Coeficiente(s) de Mel [7:12]") +
            xlab("Pista audio") + ylab("Valor Coef. Mel")

ggarrange(p_coe1, p_coe2, p_coe3, p_coe4, ncol = 2, nrow = 2)
```

Empezando por el primer coeficiente podemos observar que el género masculino tiene los valores más altos y distribuidos. Para los coeficientes que van del segundo al sexto, podemos ver que los valores femeninos van ganando relevancia según aumentamos el coeficiente, es decir, según captamos mayores frecuencias. Finalmente para los últimos coeficientes vemos los valores para los hombres son claramente inferiores y menos distribuidos.

## Frecuencia fundamental

La frecuencia fundamental es una característica ampliamente utilizada en el análisis de secuencias de audio, especialmente para diferenciar géneros y hablantes. Esto se debe a que representa el tono base de la voz, el cual tiende a ser más bajo en hombres que en mujeres. Su variabilidad entre individuos también lo convierte en un indicador relevante para la identificación de locutores. Por su relación directa con la fisiología vocal, esta métrica es clave en estudios relacionados con la prosodia y la fonética.

Su cálculo se basa en dividir la señal en ventanas (de 2 segundos en este caso) y realizar la Transformada de Fourier en cada una. La frecuencia fundamental en cada ventana se determina como la frecuencia con la máxima amplitud en el espectro resultante, considerando solo la mitad positiva del espectro. Para obtener un único valor representativo de toda la señal, se calcula la media ponderada de estas frecuencias, asignando un peso mayor a las primeras ventanas y eliminando la contribución de los últimos 4 segundos, ya que suelen ser menos relevantes debido a que el locutor ya ha terminado de hablar en la mayoría de los casos.

```{r}
# FO
# Inicializar el vector para la característica FO
FO_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  #audio_de <- audio@right
  
  # Calcular FO para cada canal
  F0_iz <- FO(audio_iz, audio@samp.rate)
  #F0_de <- FO(audio_de, audio@samp.rate)
  
  # Promediar los valores calculados
  #FO_carac[id] <- mean(c(F0_iz, F0_de))
  FO_carac[id] <- F0_iz
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica FO al marco de datos
df_caract <- cbind(df_caract, FO_carac)
```

```{r warning=FALSE}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")

# Crear el gráfico combinado
p_f0_1 <- ggplot(df, aes(x = locutor, y = FO_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Frecuencia Fundamental (F0)",
    x = "Locutor",
    y = "Frecuencia Fundamental (F0)",
    color = "Género"
  ) +
  scale_y_continuous(limits = c(75, 300)) +  # Ajustar el rango del eje Y
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

p_f0_2 <- ggplot(df, aes(x = FO_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de F0",
    x = "Frecuencia Fundamental (F0)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()


ggarrange(p_f0_1,
          p_f0_2,
          ncol = 2)
```
Nuestro caso se muestra paradójico con respecto a esta característica, ya que, aunque las mujeres presentan frecuencias fundamentales altas y similares, los hombres se distribuyen en un rango más amplio, desde frecuencias bajas hasta las más altas. Este comportamiento no es el esperable teóricamente, pues se ha demostrado que las mujeres tienen una frecuencia fundamental superior a la de los hombres, lo que en nuestro caso no ocurre.

Esta discrepancia podría explicarse por factores propios de los datos, como diferencias en las condiciones de grabación o la inclusión de locutores con características atípicas, como hombres con frecuencias fundamentales altas o mujeres con frecuencias bajas. 

## Formantes

Los formantes se encuentran caracterizados por aquellas intensidades en el espectro de sonido que destacan en un señal. Altamente relacionado con el tracto vocal de la persona, produciendo una gran cantidad de formantes. Siendo los más relevantes los primeros. En concreto los dos primeros relacionados con el sonido de las vocales. A partir del segundo formante, las frecuencias le dan color a nuestra voz, hasta el punto de tener frecuencias no distinguibles para el oído humano.

Con el objetivo de una sencilla visualización, a continuación obtendremos los formantes a partir de los sonidos de las vocales "a" y "e". Tras haber recortado algunos de los audios grabados con el objetivo de investigar la característica.

De los formantes obtenidos hemos realizado la media por género. Por lo general, esperamos valores más altos para las mujeres. Cabe destacar que al haber recortado manualmente los audios, los fonemas no son puros. Además el tamaño de nuestra muestra es bastante reducido. Es por eso que no esperamos obtener resultados perfectos, en caso de querer comparar con otros estudios, pero sí pudiendo diferenciar. 

```{r extraccion_caract_for_1}
# Vocal E
carga <- carga_audios("../Data/vocal_e")
vocales <- carga[[1]]
gen <- c(rep("Femenino", 5), rep("Masculino", 5))
f1_e <- numeric(length(vocales))
f2_e <- numeric(length(vocales))
id <- 1

for (vocal in vocales) {
  f <- findformants(vocal@left, fs=vocal@samp.rate, verify=FALSE)
  fs <- f$formant[f$formant < 2500]
  f1_e[id] <- fs[1]
  f2_e[id] <- fs[2]
  id <- id + 1
}

df_e <- data.frame(genero = gen,
                   f1 = f1_e,
                   f2 = f2_e)
df_e <- df_e %>%
  group_by(genero) %>%
  summarise(f1 = round(mean(f1, na.rm=TRUE), 2),
            f2 = round(mean(f2, na.rm=TRUE), 2))

# Vocal A
carga <- carga_audios("../Data/vocal_a")
vocales <- carga[[1]]
f1_a <- numeric(length(vocales))
f2_a <- numeric(length(vocales))
id <- 1
for (vocal in vocales) {
  f <- findformants(vocal@left, fs=vocal@samp.rate, verify=FALSE)
  fs <- f$formant[f$formant < 2000]
  f1_a[id] <- fs[1]
  f2_a[id] <- fs[2]
  id <- id + 1
}

df_a <- data.frame(genero = gen,
                   f1 = f1_a,
                   f2 = f2_a)
df_a <- df_a %>%
  group_by(genero) %>%
  summarise(f1 = mean(f1, na.rm=TRUE),
            f2 = mean(f2, na.rm=TRUE))
```

```{r analisis_for_1}
colnames(df_e) <- c("Genero", "F1 (vocal E)", "F2 (vocal E)")
colnames(df_a) <- c("Genero", "F1 (vocal A)", "F2 (vocal A)")

knitr::kable(cbind(df_a, df_e[-1]), format = "latex", 
             booktabs = TRUE, 
             caption = "Formantes en las vocales A y E", 
             align = 'lll', centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)
```

A partir de los datos obtenidos, podemos diferenciar claramente el género del locutor. Para el caso de la vocal E, los formantes 1 y 2, para las mujeres son más altos. Sin embargo, si luego observamos en el caso de la vocal A, para el formante 1, tenemos valores muy similares, es solo en el formante 2 donde podemos diferenciar. Teniendo otra vez valores inferiores en los hombres.

Debido al coste computacional implicado en el cálculo de formantes, hemos decidido precalcularlo utilizando la aplicación PRAAT. De cada audio extraemos una palabra con ciertos fonemas caraterísticos. Teniendo en cuenta la complejidad que buscamos con nuestro clasificador, bajo el objetivo de clasificar el género del locutor, resumiremos los datos. En vez de utilizar los formantes en su total extensión en el tiempo calcularemos estadísticos, como la media y la desviación estándar para los 3 primeros formantes.

```{r extraccion_caract_for_2}
df_for <- carga_formants("../Data/formants_as.txt", audios)
df_caract <- cbind(df_caract, df_for[-1])
```

Si analizamos los valores de los formantes visualizamos en parte lo esperado. Para el primer formante tenemos frecuencias más altas en las mujeres. Según pasamos a los siguientes a los siguientes formantes notamos una evolución donde las frecuencias en los hombres van tomando más protagonismo.

```{r analisis_for_2}
df_analisis_for <- df_caract %>%
  left_join(meta_audios[c("genero", "pista", "id_audio")], by = "id_audio") %>%
  dplyr::filter(pista %in% paste0("0", c(1:5)))

p_for1 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = pista, y = f1_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F1") +
            xlab("Pista audio") + ylab("Valor frecuencia")

p_for2 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = pista, y = f2_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F2") +
            xlab("Pista audio") + ylab("Valor frecuencia")

p_for3 <- ggplot(data = df_analisis_for,
                 mapping = aes(x = pista, y = f3_avg, fill = genero)) +
            geom_boxplot() +
            ggtitle("F3") +
            xlab("Pista audio") + ylab("Valor frecuencia")

ggarrange(p_for1,
          ggarrange(p_for2, p_for3, ncol = 2),
          nrow = 2)
```

## Zero Crossing Rate

La tasa de cruces por cero (ZCR, por sus siglas en inglés) mide cuántas veces la señal cruza el eje cero en un periodo de tiempo determinado. Es mayor en voces femeninas debido a su mayor frecuencia fundamental, lo que evidencia una estrecha relación entre ambas características. Este parámetro se utiliza frecuentemente en el análisis de señales para distinguir patrones relacionados con la tonalidad y la estructura armónica de la voz.

Para su cálculo, se emplea la función zcr en R, utilizando ventanas con un solapamiento del 50%. Posteriormente, se realiza el promedio de los valores obtenidos en cada ventana para obtener un único valor representativo por señal, similar al enfoque utilizado para calcular la frecuencia fundamental. Esta media permite consolidar la información de toda la señal en un solo parámetro útil para el análisis.

```{r}
# ZCR
# Inicializar el vector para la característica ZCR
ZCR_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  #audio_de <- audio@right
  
  ZCR_iz <- ZCR_FUN(audio_iz, audio@samp.rate)
  #ZCR_de <-ZCR_FUN(audio_de, audio@samp.rate)
  
  # Promediar los valores calculados
  #ZCR_carac[id] <- mean(c(ZCR_iz, ZCR_de))
  ZCR_carac[id] <- ZCR_iz
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica ZCR al marco de datos
df_caract <- cbind(df_caract, ZCR_carac)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")

# Crear el gráfico combinado
p_zcr_1 <- ggplot(df, aes(x = locutor, y = ZCR_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "Zero Crossing rate (ZCR)",
    x = "Locutor",
    y = "Zero Crossing rate (ZCR)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

p_zcr_2 <- ggplot(df, aes(x = ZCR_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de ZCR",
    x = "Zero Crossing Rate (ZCR)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

ggarrange(p_zcr_1,
          p_zcr_2,
          ncol = 2)
```

En esta característica se observa un comportamiento más acorde con lo esperado teóricamente. El Zero Crossing Rate (ZCR) tiende a ser mayor en las mujeres que en los hombres, lo cual es consistente con las propiedades de las señales de voz. Este patrón se aprecia tanto en la distribución general del ZCR por género como en los locutores individuales, donde, por norma general, las mujeres presentan valores de ZCR superiores. Este resultado refleja que el ZCR es una característica adecuada para diferenciar géneros en nuestro caso de estudio.

## Linear Predictive Coding

El Linear Predictive Coding (LPC) es un modelo matemático que describe una señal de audio en función de una combinación lineal de sus valores pasados. En lugar de representar la señal de audio directamente, se modela como una secuencia de coeficientes que predicen el valor futuro de la señal basándose en los valores previos. Los coeficientes LPC son los parámetros que caracterizan este modelo de predicción lineal. En el contexto de la LPCC, los coeficientes son derivados a partir de los coeficientes LPC mediante una transformación matemática, y son usados frecuentemente como una representación compacta y eficiente de la información espectral de la señal. Nuestro objetivo es analizar si los coeficientes obtenidos para los audios con locutores hombres tienen patrones distintos a los obtenidos para las locutoras.

Para obtener los coeficientes LPCC seguiremos los siguientes pasos:

1. Preprocesamiento de la señal: La señal de audio se segmenta en ventanas de corto tiempo (frames) para su análisis.

2. Cálculo de los coeficientes LPC: A partir de cada segmento de la señal, se calcula un conjunto de coeficientes LPC (usaremos la función lpc). Estos coeficientes describen cómo la señal futura puede ser predicha a partir de los valores anteriores.

3. Transformación a LPCC: Una vez obtenidos los coeficientes LPC, se aplican transformaciones matemáticas (como la transformada cepstral) para obtener los coeficientes LPCC. 

Usaremos orden 4, lo que significa que obtendremos 4 coeficientes por ventana. Por cada audio calcularemos la media y desviación estándar de los coeficientes.

```{r}
# Parámetros
frame_size <- 512  # Tamaño del frame
overlap <- 128     # Solapamiento
order <- 4         # Orden de LPC

# Aplicar la función a todos los audios y calcular los LPCCs
lpcc_results <- lapply(audios, function(wav) {
  # Convertir audio a vector (usando el canal 'left' para señales mono)
  signal <- as.vector(wav@left)
  calc_lpcc(signal, frame_size, overlap, order)
})

df_list <- lapply(1:length(lpcc_results), function(i) {
  # Para cada audio, expandimos cada ventana y sus coeficientes LPCC
  audio_lpcc <- lpcc_results[[i]]
  # Convertir cada ventana (lista de 4 coeficientes) en un dataframe de 4 columnas
  audio_df <- do.call(rbind, lapply(audio_lpcc, function(window) {
    data.frame(lpcc_1 = window[1], lpcc_2 = window[2], lpcc_3 = window[3], lpcc_4 = window[4])
  }))
  
  # Agregar una columna 'audio_id' para identificar el audio
  audio_df$id_audio <- i
  return(audio_df)
})

# Combinar todas las listas de dataframes en uno solo
final_df <- do.call(rbind, df_list)

df_totales <- final_df %>%
  left_join(meta_audios, by = "id_audio")
```

```{r, warning=FALSE}
df <- df_totales %>%
group_by(id_audio, genero, pista, locutor) %>%
summarise(
  lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
  lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
  lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
  lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  .groups="keep"
)

df_caract <- cbind(df_caract, df[-c(1:6)])
```

A continuación, analizaremos si es posible identificar patrones en los coeficientes que permitan discernir el género del locutor.

Vamos a filtrar por la pista de audio y veremos si se aprecian diferencias entre la media y la desviación estándar en función del género del locutor. Hemos ignorado el valor del primer coeficiente pues siempre es 1.

```{r}
# Resumen descriptivo de los coeficientes LPCC por género
# Calcular estadísticas descriptivas por género
summary_stats <- df_totales %>%
  dplyr::filter(pista == '02') %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  )

names(summary_stats) <- c("genero", paste0(rep(paste0("C", 1:4), each=2), c("_M", "_SD")))

# Suponiendo que tu dataframe se llama df_coefs
knitr::kable(summary_stats[-c(2, 3)], format = "latex", 
             booktabs = TRUE, 
             caption = "Media y desviación del LPCC para la pista 2", 
             align = 'ccccccccc',  # Ajusta según el número de columnas
             centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)

# Resumen descriptivo de los coeficientes LPCC por género
# Calcular estadísticas descriptivas por género
summary_stats <- df_totales %>%
  dplyr::filter(pista == '03') %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4),
  )

names(summary_stats) <- c("genero", paste0(rep(paste0("C", 1:4), each=2), c("_M", "_SD")))

# Suponiendo que tu dataframe se llama df_coefs
knitr::kable(summary_stats[-c(2, 3)], format = "latex", 
             booktabs = TRUE, 
             caption = "Media y desviación del LPCC para la pista 3", 
             align = 'ccccccccc',  # Ajusta según el número de columnas
             centering = TRUE,
             label = NA,
             table.envir = "table", position = "H") %>%
  kable_styling(position = "center", full_width = FALSE)
```

Notamos que ciertos patrones se repiten para todas las pistas de audio. La media de los coeficientes 2 y 4 correspondientes a los chicos es mayor que la de las chicas en todas las pistas. Mientras que con el coeficiente 3 pasa al contrario. También podemos destacar que la desviación estándar de los coeficientes es en todos los casos superior para los locutores chicos que para las chicas. 

## Short-Time Energy

La característica Short-Time Energy (STE) mide la energía contenida en una señal dentro de pequeñas ventanas temporales. Puede ser muy útil para analizar variaciones de energía a lo largo del tiempo. Para nuestro caso, puede ser particularmente interesante porque las variaciones de energía a lo largo del tiempo pueden reflejar características importantes de la voz humana, como la intensidad, los patrones de habla y las pausas.

El procedimiento que hemos seguido consiste en tomar cada señal (audio) y dividirla en ventanas temporales. Luego, calculamos la energía cuadrática media para cada ventana y la normalizamos. Finalmente, promediamos los valores de STE normalizados obtenidos en cada audio para obtener un único valor representativo.

```{r extraccion_caract_ste}
ste_c <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  ste_c[id] <- mean(STE(audio, audio@samp.rate * 0.02, 50)[,2])
  id = id + 1
}

# Agregar caracteristica
df_caract <- cbind(df_caract, ste_c)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")

p_ste_1 <- ggplot(df, aes(x = locutor, y = ste_c, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3) +  # Boxplots por género
  labs(
    title = "Short-Time Energy (STE)",
    x = "Locutor",
    y = "Short-Time Energy (STE)",
    color = "Género"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p_ste_2 <- ggplot(df, aes(x = ste_c, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de STE",
    x = "Short-Time Energy (STE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

ggarrange(p_ste_1,
          p_ste_2,
          ncol = 2)
```

Observamos que las locutoras tienen una mayor variabilidad en la STE en comparación con los locutores hombres. Esto podría deberse a características propias de las voces femeninas, como el rango de modulación, que es más amplio. Además, las locutoras presentan valores de STE más altos en general. Esto se refleja en una mediana de aproximadamente 0.10, mientras que en los locutores hombres la mediana se encuentra alrededor de 0.07. Esto puede tener relación con la frecuencia fundamental. Las voces femeninas suelen tener una frecuencia fundamental más alta que las voces masculinas. Dado que la STE mide la energía de la señal acústica, una frecuencia fundamental más alta puede generar más energía en las frecuencias superiores, lo que puede reflejarse en un valor más alto de STE.

Las curvas de densidad muestran que las voces femeninas tienen una distribución más amplia y desplazada hacia valores más altos de STE, lo que confirma lo que hemos visto en el boxplot. Los locutores hombres, por otro lado, presentan una distribución más concentrada y con menor dispersión. Aunque hay diferencias entre géneros, también se observa cierto solapamiento en las densidades de STE. Esto indica que, si bien hay tendencias generales, la STE por sí sola puede no ser completamente discriminativa entre géneros en todos los casos.

En conclusión, los resultados sugieren que la Short-Time Energy es una métrica que capta diferencias significativas entre géneros, aunque también está influenciada por características individuales.

## Piecewise Gaussian Modeling

El PGM es un método que utiliza una estructura a largo plazo para representar las características, llamadas Ventanas de Tiempo de Integración (ITW). En cada ITW, se tiene un vector de medias y un vector de varianzas.

Para la implementación de este método, primero calculamos el MFSC (Mel Frequency Spectral Coefficient). Es similar al MFCC obtenido anteriormente pero sin la parte de DCT (Discrete Cosine Transform). Una vez calculado, se obtiene una matriz $N \times T$ donde $T$ se refiere al número de vectores espectrales contenidos en una ITW (Ventana de Tiempo de Integración) y N al número de ventanas ITW. A continuación se modela un conjunto de $T$ vectores MFSC consecutivos mediante un modelo gaussiano. Es decir, $N \times T$ de MFSC serán modelados por N gaussianas. Al calcular la matriz de covarianza, solo tomamos el índice diagonal de la matriz de covarianza para generar el vector de varianza. Finalmente, las características del PGM son la concatenación de la media y la varianza normalizadas por sus respectivos máximos y mínimos. Para obtener una característica de media y otra de varianza por audio, hemos calculado la media de cada vector.

```{r extraccion_caract_PGM, message=FALSE, warning=FALSE}
# PGM
# Inicializar listas para almacenar los resultados
mean_charac <- numeric(length(audios))
var_charac <- numeric(length(audios))

# ID para iterar sobre los audios
id <- 1
for (audio in audios_norm) {
  PGM_left <- PGM(audio, coef=172) 
  
  # Almacenar los resultados en las listas
  mean_charac[id] <- mean(PGM_left$medias)
  var_charac[id] <- mean(PGM_left$varianzas)
  
  # Incrementar el contador
  id <- id + 1
}

# Crear un data frame con los resultados
PGM_caract <- data.frame(
  pgm_mean = mean_charac,
  pgm_var = var_charac
)

# Agregar la característica PGM al marco de datos
df_caract <- cbind(df_caract, PGM_caract)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")

p_pgm1 <- ggplot(df, aes(x = pgm_mean, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución asociada a vectores de medias por Género",
    x = "Medias normalizadas",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_pgm2 <- ggplot(df, aes(x = pgm_var , fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución asociada a vectores de varianzas por Género",
    x = "Varianzas normalizadas",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()


ggarrange(p_pgm1, p_pgm2, nrow=2)
```

Las curvas de densidad muestran que para la característica de las medias normalizada, las voces masculinas tienen una menor dispersión que las femeninas y un mayor valor en promedio, pero parecen mezclarse entre sí en un mismo rango de valores. A priori no parece una buena característica para discriminar

La característica asociada a las varianzas normalizadas podría ser un mejor discriminante tampoco parece una buena característica para discrimanr. La dispersión de las voces masculinas es muy grande, asemejandose mucho a una distribución uniforme continua (prácticamente no aportan información). En cambio, la dispersión de las voces femeninas es menor y los valores se concentran mas en torno a la media.

## Power Spectrum

Esta característica nos indica qué tan fuertes están presentes diferentes frecuencias en una señal. Para estimar el Power Spectrum, se utiliza un estimador llamado periodograma. Los pasos a seguir son los siguientes:

- **Transformada Rápida de Fourier (FFT):** Se aplica la transformada para cambiar al dominio de las frecuencias.

- **Cálculo del periodograma:** Se calcula el módulo al cuadrado de la transformada de Fourier en cada frecuencia. El resultado es el periodograma, que representa la distribución de la potencia de la señal en función de la frecuencia.

- **Frecuencia de máxima potencia en el espectro:** Se identifica la frecuencia en la que se encuentra el valor máximo del espectro de potencia. Este valor de potencia máxima, junto con su frecuencia asociada, se utilizan como características clave para la clasificación de cada audio.

```{r extraccion_caract_PS}
# PS
# Inicializar listas para almacenar los resultados
max_potencias <- numeric(length(audios))
freq_max_potencias <- numeric(length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios_norm) {
  # Acceso directo a los canales izquierdo y derecho

  # Calcular PS para canal izquierdo
  PS_left <- PS(audio@left, audio@samp.rate, span=9) #Se puede modificar el span
  
  # Almacenar los resultados en las listas
  max_potencias[id] <- max(PS_left$Potencia)
  freq_max_potencias[id] <- PS_left$Frecuencia[which.max(PS_left$Potencia)]
  
  # Incrementar el contador
  id <- id + 1
}

# Crear un data frame con los resultados
PS_caract <- data.frame(
  ps_pow = max_potencias,
  ps_frec = freq_max_potencias
)

# Agregar la característica PS al marco de datos
df_caract <- cbind(df_caract, PS_caract)
df <- df_caract %>%
  left_join(df, by = "id_audio") 
```

```{r}
p_for1 <- ggplot(df, aes(x = ps_frec, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de frecuencia asociada a potencia máxima por Género",
    x = "Frecuencia máxima",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_for2 <- ggplot(df, aes(x = ps_pow , fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de potencia máxima por Género",
    x = "Potencia máxima",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()


ggarrange(p_for2 ,p_for1, nrow=2)
```


Se puede observar una mayor concentración de valores más bajos de potencia máxima en las voces masculinas mientras que en las voces femeninas se muestra una distribución más dispersa, con algunos valores más altos de potencia máxima.

La distribución de la frecuencia de máxima potencia en el espectro, a priori, parece un poco mas determinante para la tarea de clasificación. En las voces femeninas hay una mayor densidad en frecuencias medias/altas (alrededor de 190 Hz) mientras que en las voces masculinas pueden diferenciarse dos grupos de frecuencias (bajas y altas), por lo que no parecen estar tan concentradas alrededor de una única frecuencia media.

```{r, include=FALSE}
# Crear la nube de puntos
ggplot(df, aes(x = ps_frec, y = ps_pow, color = genero, shape = genero)) +
  geom_point(size = 3) +
  labs(
    x = "Frecuencia asociada a máxima potencia (Hz)",
    y = "Máxima Potencia",
    title = "Nube de Puntos",
    color = "Género",
    shape = "Género"
  ) +
  theme_minimal()
```

## Harmonic-to-Noise Ratio (HNR)

Esta característica mide la proporción entre la energía de la componente armónica de la señal respecto al ruido presente. Aunque su principal uso es en el área de la sanidad para detectar patologías mediante la voz, también nos puede ayudar a distinguir la claridad y ciertas particularidades de las voces.

Para ello, nos ayudaremos de la descomposición mediante wavelets para separar las Aproximaciones (componente de baja frecuencia de la señal normalmente interpretada como la parte harmónica) de los Detalles (componente de alta frecuencia relacionada con el ruido).

```{r extraccion_caract_HNR, warning=FALSE}
HNR_carac <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  # Calcular HNR para la señal y almacenarla
  HNR_carac[id] <- HNR(audio)
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica HNR al marco de datos
df_caract <- cbind(df_caract, HNR_carac)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")

p_hnr1 <- ggplot(df, aes(x = HNR_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución HNR por Género",
    x = "Harmonic-to-Noise Ratio (HNR)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_hnr2 <- ggplot(df, aes(x = locutor, y = HNR_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "HNR por Locutor y Género",
    x = "Locutor",
    y = "Harmonic-to-Noise Ratio (HNR)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggarrange(p_hnr2, p_hnr1, ncol=2)
```

El gráfico de distribución de densidades muestra una clara diferencia de tendencia entre géneros; mientras que las voces masculinas suelen encontrarse más cerca de la media cercana a 62, el HNR de las voces femeninas está distribuido de forma más uniforme en el rango de valores. De esta forma, valores de HNR cercanos a los extremos podrían indicar mayor probabilidad de que la voz grabada sea femenina y valores cercanos a la media lo contrario. No obstante, esta diferencia de tendencias según el género no es muy evidente o conlcuyente, por lo que este indicador por si mismo puede que no sea suficiente pero potencialmente puede aportar información relevante a un algoritmo de clasificación.

Centrándonos en el HNR según locutor, podemos ver que en el caso del género femenino hay cierta constancia en los valores, agrupándose en un rango no muy amplio que podría ayudar a su identificación. En cuánto al género masculino, sus HNR se encuentran distribuidos en intervalos semejantes por lo que podría ser considerablemente difícil discernir entre locutores masculinos.

## Centroides Espectrales (CE)

Esta medida pretende caracterizar el espectro del audio señalando el "centro de masa" de su energía. Dicho centro se calcula realizando la media ponderada de las frecuencias encontradas usando una transformada de Fourier y usando sus amplitudes como pesos. 

El centroide espectral de un audio tiene profunda relación con el timbre del mismo, en particular con su "brightness"; esta característica puede ser muy útil a la hora de diferenciar entre géneros e incluso de identificar patrones únicos en cada locutor.

Para su implementación en R nos ayudaremos de la librería seewave, la cual cuenta con esta medida dentro del resumen de estadísticas presentes en la función "specprop".

```{r extraccion_caract_CE, warning=FALSE}
CE_carac <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  # Calcular CE para la señal y almacenarla
  CE_carac[id] <- CE(audio)
  
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica CE al marco de datos
df_caract <- cbind(df_caract, CE_carac)
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")

p_ce1 <- ggplot(df, aes(x = CE_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución CE por Género",
    x = "Centroide Espectral (CE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_ce2 <- ggplot(df, aes(x = locutor, y = CE_carac, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +  # Boxplots por género
  labs(
    title = "CE por Locutor y Género",
    x = "Locutor",
    y = "Centroide Espectral (CE)",
    color = "Género"
  )  +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggarrange(p_ce2, p_ce1, ncol=2)
```

Notamos un comportamiento muy semejante al de la característica Harmonic-to-Noise Ratio (HNR): los CE de las voces masculinas se encuentran mucho más concentradas en torno a una media cercana a 175 mientras que las voces femeninas se reparten de forma más homogénea a lo largo del rango total 100-260, teniendo dos máximos locales en 120 y 230.

En cuánto al análisis por locutor, podemos observar considerable variabilidad de centroides espectrales dependiendo del audio (salvo en el caso del locutor 01), lo que puede dificultar el uso de esta medida para la identificación concreta del locutor. Sin embargo, al igual que en características anteriores la diferencia de distribución de densidad entre géneros puede aportar información relevante a la hora de clasificarlos.

## Shimmer

La característica Shimmer mide la variabilidad de ciclo a ciclo de la amplitud en una señal de voz. Esta variabilidad se ve afectada por la tensión de las cuerdas vocales y factores fisiológicos.

Para capturar esta variabilidad en primer lugar diferenciamos la señal y normalizamos estas diferencias según las amplitudes de la señal original. Despues realizamos la media de estas diferencias y por ultimo transformamos el resultado de decibels.

```{r extraccion_caract_SH}
# SH
# Inicializar el vector para la característica SH
SH_carac <- rep(0.0, length(audios))

# ID para iterar sobre los audios
id <- 1

for (audio in audios) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio@left
  #audio_de <- audio@right
  
  SH_iz <- SH(audio_iz)
  #SH_de <-SH(audio_de)
  
  # Promediar los valores calculados
  SH_carac[id] <- SH_iz
  
  # Incrementar el contador
  id <- id + 1
}

# Normalizados
SH_carac_norm <- rep(0.0, length(audios))
id <- 1

for (audio2 in audios_norm) {
  # Acceso directo a los canales izquierdo y derecho
  audio_iz <- audio2@left
  #audio_de <- audio2@right
  
  SH_iz <- SH(audio_iz)
  #SH_de <-SH(audio_de)
  
  # Promediar los valores calculados
  SH_carac_norm[id] <- SH_iz
  # Incrementar el contador
  id <- id + 1
}

# Agregar la característica SH al marco de datos
df_caract <- cbind(df_caract, SH_carac)
df_caract <- cbind(df_caract, SH_carac_norm)

df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")
```

```{r}
p_s1 <- ggplot(df, aes(x = SH_carac, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de SH por Género sin Normalizar",
    x = "SH",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

p_s2 <- ggplot(df, aes(x = SH_carac_norm, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de SH por Género y Normalizados",
    x = "SH",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

ggarrange(p_s1, p_s2, nrow=2)
```

Podemos observar en el grafico de densidad diferencias en la distribución por genero. La distribucion de SH para mujeres parece estar mas concentrada, tanto en los datos que se han normalizado y los que no, mientras que la distribucion para hombres parece estar más dispersa. Estas diferencias podrían reflejar características fisiológicas de las cuerdas vocales.

Aunque se observan diferencias claras entre géneros en términos de rango y dispersión de SH, también es evidente que hay un solapamiento y variabilidad entre locutores del mismo género.

Esto indica que SH puede ser una característica útil para clasificación de locutores dentro del mismo género o entre géneros, pero debería combinarse con otras métricas para obtener resultados más precisos.

## Perceptual Linear Prediction

El Perceptual Linear Prediction (PLP) es una técnica de extracción de características basada en principios psicoacústicos que incorpora el modelo de predicción lineal (LPC) para representar señales acústicas de forma robusta y compacta. Aunque comparte similitudes con el cálculo tradicional de coeficientes LPC, PLP introduce modificaciones inspiradas en la percepción humana del sonido, ajustando el espectro de la señal para reflejar cómo el oído humano procesa y percibe las frecuencias.

Para obtener los coeficientes LPCC seguiremos los siguientes pasos:

1. Calculamos el espectrograma utilizando una ventana Hamming.
2. Extraer la densidad espectral de potencia (PSD).
3. Banco de filtros Bark: Filtramos la señal mediante un banco de filtros basado en las bandas críticas percibidas por el oído humano.
4. Aplicamos un filtro de pre-énfasis para ajustar las amplitudes de frecuencias para reflejar cómo el oído humano percibe las distintas intensidades a diferentes frecuencias.
5. Aplicamos una transformación para simular la potencia percibida por el oído humano. Cada elemento espectral se eleva a la potencia de 0.33. Este enfoque se utiliza en lugar de una transformación logarítmica porque el logaritmo tiende a comprimir de forma más agresiva los valores altos, mientras que la raíz cúbica proporciona una representación más fiel de la percepción auditiva, especialmente en señales con variaciones amplias de intensidad.
6. Cálculo de coeficientes LPC: A partir de cada segmento de la señal(usaremos la función lpc)
7. Calcular los coeficientes cepstrales (LPCCs): Transformamos los coeficientes LPC en coeficientes cepstrales LPCCS.

Por lo tanto, la dimensión de salida de nuestras características dependerá del orden seleccionado para los modelos lineales, así como del tamaño de la ventana y el solapamiento utilizado. En nuestro caso, siguiendo recomendaciones generales, obtenemos una salida aproximadamente de 14X6,247.  Para compactar aún más la información, calcularemos la media de cada coeficiente, lo que nos dará un vector de longitud 14.
A la hora de aplicar modelos, podríamos explorar tanto el uso de la matriz completa como el vector compacto que resume la información. Sin embargo, para la visualización de los valores de los coeficientes según el locutor, emplearemos el vector compacto.

```{r extraccion_caract_PLP}
# Guardamos cada coeficiente de la salida como una columna nueva excluyendo el primer coeficiente pues siempre es 0
num_features <- 14
PLP_matrix <- matrix(0, nrow = length(audios), ncol = num_features-1)

# Caracteristica PLP como lista para almacenar el vector en una columna
# Puede ser util en los modelos de ml
PLP_carac <- vector("list", length(audios))
PLP_carac_matriz <- vector("list", length(audios))

id <- 1

for (audio in audios) {
  # Calcular PLP para cada canal
  
  PLP_iz <- PLP(audio, wlen = 256, ovlp = 70, side = 'l')
  # PLP_de <- PLP(audio, wlen = 256, ovlp = 70, side = 'r')
  
  # Promediar los valores calculados
  # PLP_med <- (PLP_iz + PLP_de) / 2
  
  # Almacenar el resultado en la lista
  PLP_carac_matriz[[id]] <- PLP_iz
  
  PLP_iz <- rowMeans(PLP_iz)
  #PLP_de <- rowMeans(PLP_de)
  
  # Promediar los valores calculados
  # PLP_med <- (PLP_iz + PLP_de) / 2
  
  PLP_matrix[id, ] <- PLP_iz[-1]  # Primer coeficiente siempre 0
  PLP_carac[[id]] <- PLP_iz
  
  id <- id + 1
}

colnames(PLP_matrix) <- paste0("PLP_c", 2:num_features)

# Combinar el nuevo marco de datos con el original
df_caract <- cbind(df_caract, PLP_matrix)

# df_caract$PLP_carac <- PLP_carac
# df_caract$PLP_carac_matriz <- PLP_carac_matriz

# Verificar el resultado
# print(head(df_caract))

df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")
```

```{r}
# Mostramos primeros 4
names_plp <- paste0("PLP_c", 2:5)
graficas <- list()
id <- 1

for (i in names_plp) {
  graficas[[id]] <- ggplot(df, aes(x = locutor, y = .data[[i]], color = genero)) +
      # Puntos
      geom_point(size = 3, alpha = 0.8) +
      # Boxplots por género
      geom_boxplot(aes(group = genero), alpha = 0.3, outlier.shape = NA) +
      labs(
        title = paste("Distribución de", i),
        x = "Locutor",
        y = i,
        color = "Género"
      ) +  
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  id <- id + 1
}

ggarrange(ggarrange(graficas[[1]], graficas[[2]], nrow=2),
          ggarrange(graficas[[3]], graficas[[4]], nrow=2),
          ncol = 2)
```

Las distribuciones muestran una clara diferencia entre locutores, aunque no se observa una distinción significativa entre géneros al analizar un solo coeficiente en particular. Esto sugiere que el coeficiente individual podría ser útil para distinguir entre locutores. Sin embargo, al considerar los 14 coeficientes en conjunto, es probable que se puedan obtener mejores resultados para la clasificación por género.

```{r guardar_caracteristicas}
save(df_caract, file = "../Data/caracteristicas_precalculadas.RData")
```

# Selección de características

En el siguiente apartado vamos a probar distintas formas de seleccionar características. Con el objetivo de clasificar mejor sin la necesidad de utilizar todos los recursos. Buscando obtener la máxima relevancia con el objetivo y la menor redundancia.

```{r agregar_etiquetas}
load("../Data/caracteristicas_precalculadas.RData")
df_etiquetado <- df_caract %>%
  left_join(meta_audios %>% select(id_audio, genero, locutor), by = "id_audio")
```

## Importancias de las características usando RandomForest

En este apartado lanzaremos 1000 random forest, de los cuales obtendremos la importancia de las características. Quedándonos con las 10 mejores características. Siendo estas:

```{r}
X_sel <- df_etiquetado[!(names(df_etiquetado) %in%
                                       c("genero", "locutor", "id_audio"))]
Y_sel <- as.factor(df_etiquetado$genero)

# 1. Qué relevancia tienen las características en el RFC?
library(randomForest)

# Entrenar el modelo de Random Forest
set.seed(123) # Fijar la semilla para reproducibilidad

names_carac <- names(df_etiquetado)
vec_imp_f <- numeric(length(names_carac))
vec_imp_m <- numeric(length(names_carac))
names(vec_imp_f) <- names_carac
names(vec_imp_m) <- names_carac

for (i in 1:1000) {
  rf_sel_carac <-
    randomForest(x = X_sel,
                 y = Y_sel,
                 importance = TRUE,
                 ntree = 20)
  
  vec_imp_f <- vec_imp_f + abs(rf_sel_carac[["importance"]][,1])
  vec_imp_m <- vec_imp_f + abs(rf_sel_carac[["importance"]][,2])
}
vec_total <- vec_imp_f + vec_imp_m
rf_1000_imp <- names(head(sort(vec_total, decreasing=TRUE), 10))
```
`r rf_1000_imp`.

## Técnicas recursivas de eliminación de características (RFE)

Sin embargo, utilizaremos el RFE. Este consistirá en ejecutar algoritmos de selección de características que irá eliminando recursivamente las menos importantes. Probaremos a obtener las 10 mejores características.

```{r}
# 2. Recursive Feature Elimination RFE
library(caret)
r_control <- rfeControl(functions=rfFuncs,
                        method="cv",
                        number=10)

rfe_result <- rfe(X_sel, Y_sel, sizes= c(1:10), rfeControl=r_control)
```

El método devuelve que el número ideal de características es `r length(rfe_result$optVariables)`, siendo las variables más relevantes `r rfe_result$optVariables`.

## Correlaciones

```{r}
# 3. Correlaciones
df_numerico <-
  df_etiquetado[!(names(df_etiquetado) %in% c("genero", "locutor", "id_audio"))] %>%
  select(rfe_result$optVariables)

matriz_correlacion <- cor(df_numerico)

carac_sel <- names(head(sort(colSums(abs(matriz_correlacion)), decreasing=FALSE),
                        length(rfe_result$optVariables)/2))
```

De las características calculadas previamente vamos a reducirlas a la mitad. Para ello utilizaremos las menos correlacionadas entre sí. Siendo estas: `r carac_sel`.

# Algoritmos de ML

El primer paso antes de hacer ningún algoritmo de ML es añadir a nuestro df de características las posibles etiquetas de estos algoritmos. En este caso las etiquetas se encuentran en el df meta_audios, siendo estas el locutor y el género.

De los algoritmos empleados hemos decidido utilizar un modelo más simple, que en este caso es el Random Forest, y uno más complejo; el SVM. Para evaluar como se compartan a partir de las características extraídas.

## Métodos empleando como etiqueta el género 

Primeramente, preparamos los datos separándolos en conjuntos de train y test. Es fundamental agrupar por género para garantizar un balance adecuado, evitando que un conjunto (por ejemplo, train) tenga una mayoría desproporcionada de un género, como "M", y el otro conjunto, como test, tenga una mayoría de "F". Este enfoque asegura una representación equitativa de ambos géneros en los datos de entrenamiento y prueba.

Se han empleado las características obtenidas de la selección.

```{r}
set.seed(123) # Fijar la semilla para reproducibilidad

# Crear los conjuntos de train y test balanceados por el género
train_test_split <- df_etiquetado %>%
  group_by(genero) %>%
  group_split()

train <- bind_rows(
  train_test_split %>% map(~ .x %>% sample_frac(0.75))
)

test <- anti_join(df_etiquetado, train, by = "id_audio")
```

Ahora separamos las etiquetas del conjunto de características de train

```{r}
# Mezclar aleatoriamente los datos de train y test (es un shuffle )
train <- train %>% slice_sample(prop = 1)
test <- test %>% slice_sample(prop = 1)

# Separar las características y la etiqueta

# Variables predictoras
X_train <- train %>% select(-genero, -id_audio, -locutor) %>%
  select(carac_sel)

# Etiqueta
y_train <- train$genero

X_test <- test %>% select(-genero, -id_audio, -locutor) %>%
  select(carac_sel)
y_test <- test$genero

```

### Random forest

Para el cual obtenemos la siguiente matriz de confusión:

```{r}
library(randomForest)
# Entrenar el modelo de Random Forest
rf_model <- randomForest(x = X_train,
                         y = as.factor(y_train), ntree = 20)

# Evaluar el modelo en el conjunto de test
rf_predictions <- predict(rf_model, X_test)
cm <- confusionMatrix(as.factor(rf_predictions), as.factor(y_test))
print(cm$table)
```

### SVM

Para el cual obtenemos la siguiente matriz de confusión:

```{r}
# Entrenar el modelo SVM
svm_model <- svm(x = X_train, y = as.factor(y_train), kernel = "linear")

# Evaluar el modelo en el conjunto de test
svm_predictions <- predict(svm_model, X_test)
cm <- confusionMatrix(as.factor(svm_predictions), as.factor(y_test))
print(cm$table)
```

### Conclusiones

A partir de las matrices de confusión podemos observar lo siguiente:

1. Obtenemos un 100% de acierto con las características escogidas.
2. No es necesario emplear un modelo complejo para la clasificación por género. Ya que los resultados del Random Forest son los mismo que los del SVM.

## Métodos empleando como etiqueta el locutor 

Realizamos el mismo proceso anterior para dividir el conjunto de entrenamiento de forma equitativa entre los locutores. Esta vez, vamos a emplear todas las características. De los resultados mencionar que ya no obtenemos resultados perfectos. Obteniendo peores con el Random Forest que utilizando el SVM.

```{r}
set.seed(123) # Fijar la semilla para reproducibilidad

# Crear los conjuntos de train y test balanceados por el género
train_test_split <- df_etiquetado %>%
  group_by(locutor) %>%
  group_split()

train <- bind_rows(
  train_test_split %>% map(~ .x %>% sample_frac(0.75))
)

test <- anti_join(df_etiquetado, train, by = "id_audio")

# Mezclar aleatoriamente los datos de train y test (es un shuffle )
train <- train %>% slice_sample(prop = 1)
test <- test %>% slice_sample(prop = 1)

# Separar las características y la etiqueta

# Variables predictoras
X_train <- train %>% select(-genero, -id_audio, -locutor)

# Etiqueta
y_train <- train$locutor

X_test <- test %>% select(-genero, -id_audio, -locutor)
y_test <- test$locutor
```

```{r}
# Entrenar el modelo de Random Forest
rf_model <- randomForest(x = X_train,
                         y = as.factor(y_train), ntree = 20)

# Evaluar el modelo en el conjunto de test
rf_predictions <- predict(rf_model, X_test)
cm_rf <- confusionMatrix(as.factor(rf_predictions), as.factor(y_test))

# Entrenar el modelo SVM
svm_model <- svm(x = X_train, y = as.factor(y_train), kernel = "linear")

# Evaluar el modelo en el conjunto de test
svm_predictions <- predict(svm_model, X_test)
cm_svm <- confusionMatrix(as.factor(svm_predictions), as.factor(y_test))
```

Obteniendo un Accuracy de `r cm_rf$overall["Accuracy"]` para el RF y Accuracy de `r cm_svm$overall["Accuracy"]` para el SVM. 
