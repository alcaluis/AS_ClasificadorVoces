---
title: Proyecto de Clasificación de Voces de Personas
author:
  - name: Luis Albacete Caballero
    email: alcaluis@alumni.uv.es
    affiliation: UV
  - name: Julio García Bustos
    email: jugarbus@alumni.uv.es
    affiliation: UV
  - name: Gabriel Ivars Asensio
    email: bob@example.com
    affiliation: UV
  - name: Noé López García
    email: bob@example.com
    affiliation: UV
  - name: José Miguel Palazón Caballero
    email: bob@example.com
    affiliation: UV
  - name: Joan Pedro Bruixola
    email: jopebrui@alumni.uv.es
    affiliation: UV
address:
  - code: UV
    organization: Universitat de València
    addressline: Avinguda de l'Universitat
    city: Burjassot
    state: Valencia
    postcode: 46100
    country: España
abstract: |
  Esto es un ejemplo de texto en la sección "Abstract".
keywords: 
  - Voces
  - Características
  - Características de la voz
  - Clasificación
  - Aprendizaje Máquina
  - Análisis de Señales
  - Identificación voces
date: "`r Sys.Date()`"
linenumbers: false
numbersections: true
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE,
                      #warning=FALSE,
                      out.width = "80%",
                      fig.align = "center")

# Cargar librerías
librerias <- c("tuneR",    # Lectura de archivos WAV
               "seewave"   # Manipulación mediante wavelets
              )
pacman::p_load(char = librerias)

# Cargar utilidades
source("../Utils/extraccion_caracteristicas.R")
source("../Utils/carga_datos.R")
```

```{r limpieza_audios}
# Cargar datos + Metadatos
# w <- readWave("../Data/whatstheweatherlike.wav") # Audio ejemplo
carga <- carga_audios("../Data/Audios/")
audios <- carga[[1]]
meta_audios <- carga[[2]]

library(wavelets)

# 1. Eliminación ruido
audios <- limpieza_senales(audios)

# 2. Normalización
audios <- normalizacion(audios)

# 3. DataFrame características
df_caract <- data.frame("id_audio" = meta_audios$id_audio)
```

# Característica de la voz

## Característica ejemplo

```{r extraccion_caract_ejemplo}
# WIP
ste_c <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  ste_c[id] <- mean(STE(audio, audio@samp.rate * 0.02, 50)[,2])
  id = id + 1
}

# Agregar caracteristica
df_caract <- cbind(df_caract, ste_c)
```

## Característica 1

A continuación, vamos a extraer la Short-Time Energy (STE) de cada uno de nuestros audios y analizaremos las diferencias para cada locutor y cada género. La Short-Time Energy (STE) mide la energía contenida en una señal dentro de pequeñas ventanas temporales. Puede ser muy útil para analizar variaciones de energía a lo largo del tiempo. Para nuestro caso, puede ser particularmente interesante porque las variaciones de energía a lo largo del tiempo pueden reflejar características importantes de la voz humana, como la intensidad, los patrones de habla y las pausas.


El procedimiento que hemos seguido consiste en tomar cada señal (audio) y dividirla en ventanas temporales. Luego, calculamos la energía cuadrática media para cada ventana y la normalizamos. Finalmente, promediamos los valores de STE normalizados obtenidos en cada áudio para obtener un único valor representativo por audio.


```{r extraccion_caract_1}
# WIP
ste_c <- rep(0.0, length(audios))
id <- 1

for (audio in audios) {
  ste_c[id] <- mean(STE(audio, audio@samp.rate * 0.02, 50)[,2])
  id = id + 1
}

# Agregar caracteristica
df_caract <- cbind(df_caract, ste_c)
```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor, levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
 

```

```{r}

ggplot(df, aes(x = locutor, y = ste_c, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3) +  # Boxplots por género
  labs(
    title = "Short-Time Energy (STE) por Locutor y Género",
    x = "Locutor",
    y = "Short-Time Energy (STE)",
    color = "Género"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))




```

Observamos que las locutoras tienen una mayor variabilidad en la STE en comparación con los locutores hombres. Esto podría deberse a características propias de las voces femeninas, como el rango de modulación, que es más amplio. Además, las locutoras presentan valores de STE más altos en general. Esto se refleja en una mediana de aproximadamente 0.10, mientras que en los locutores hombres la mediana se encuentra alrededor de 0.07. Esto puede tener relación con la frecuencia fundamental. Las voces femeninas suelen tener una frecuencia fundamental más alta que las voces masculinas. Dado que la STE mide la energía de la señal acústica, una frecuencia fundamental más alta puede generar más energía en las frecuencias superiores, lo que puede reflejarse en un valor más alto de STE.



```{r}

ggplot(df, aes(x = ste_c, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de STE por Género",
    x = "Short-Time Energy (STE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```


Las curvas de densidad muestran que las voces femeninas tienen una distribución más amplia y desplazada hacia valores más altos de STE, lo que confirma lo que hemos visto en el boxplot. Los locutores hombres, por otro lado, presentan una distribución más concentrada y con menor dispersión. Aunque hay diferencias entre géneros, también se observa cierto solapamiento en las densidades de STE. Esto indica que, si bien hay tendencias generales, la STE por sí sola puede no ser completamente discriminativa entre géneros en todos los casos.

En conclusión, los resultados sugieren que la Short-Time Energy es una métrica que capta diferencias significativas entre géneros, aunque también está influenciada por características individuales.








## Característica 2


El Linear Predictive Coding (LPC) es un modelo matemático que describe una señal de audio en función de una combinación lineal de sus valores pasados. En lugar de representar la señal de audio directamente, se modela como una secuencia de coeficientes que predicen el valor futuro de la señal basándose en los valores previos. Los coeficientes LPC son los parámetros que caracterizan este modelo de predicción lineal. En el contexto de la LPCC, los coeficientes son derivados a partir de los coeficientes LPC mediante una transformación matemática, y son usados frecuentemente como una representación compacta y eficiente de la información espectral de la señal.

Para obtener los coeficientes LPCC seguiremos los siguientes pasos:

1) Preprocesamiento de la señal: La señal de audio se segmenta en ventanas de corto tiempo (frames) para su análisis.

2) Cálculo de los coeficientes LPC: A partir de cada segmento de la señal, se calcula un conjunto de coeficientes LPC (usaremos la función lpc). Estos coeficientes describen cómo la señal futura puede ser predicha a partir de los valores anteriores.

3) Transformación a LPCC: Una vez obtenidos los coeficientes LPC, se aplican transformaciones matemáticas (como la transformada cepstral) para obtener los coeficientes LPCC. 

```{r}

# Parámetros
frame_size <- 512  # Tamaño del frame
overlap <- 128     # Solapamiento
order <- 4        # Orden de LPC

# Aplicar la función a todos los audios y calcular los LPCCs
lpcc_results <- lapply(audios, function(wav) {
  signal <- as.vector(wav@left)  # Convertir audio a vector (usando el canal 'left' para señales mono)
  calc_lpcc(signal, frame_size, overlap, order)
})


```

```{r}

library(tidyr)

# Usar 'lapply' para aplanar las listas dentro de 'lpcc_results'
df_list <- lapply(1:length(lpcc_results), function(i) {
  # Para cada audio, expandimos cada ventana y sus coeficientes LPCC
  audio_lpcc <- lpcc_results[[i]]
  # Convertir cada ventana (lista de 4 coeficientes) en un dataframe de 4 columnas
  audio_df <- do.call(rbind, lapply(audio_lpcc, function(window) {
    data.frame(lpcc_1 = window[1], lpcc_2 = window[2], lpcc_3 = window[3], lpcc_4 = window[4])
  }))
  
  # Agregar una columna 'audio_id' para identificar el audio
  audio_df$id_audio <- i
  return(audio_df)
})

# Combinar todas las listas de dataframes en uno solo
final_df <- do.call(rbind, df_list)

df <- final_df %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor, levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
 

```

```{r}

# Resumen descriptivo de los coeficientes LPCC por género
library(dplyr)

# Calcular estadísticas descriptivas por género
summary_stats <-df %>%
  group_by(genero) %>%
  summarise(
    lpcc_1_mean = mean(lpcc_1), lpcc_1_sd = sd(lpcc_1),
    lpcc_2_mean = mean(lpcc_2), lpcc_2_sd = sd(lpcc_2),
    lpcc_3_mean = mean(lpcc_3), lpcc_3_sd = sd(lpcc_3),
    lpcc_4_mean = mean(lpcc_4), lpcc_4_sd = sd(lpcc_4)
  )

# Ver las estadísticas
summary_stats


```

```{r}

# Parámetros
frame_size <- 512  # Tamaño del frame
overlap <- 128     # Solapamiento
order <- 8        # Orden de LPC

remove_silence <- function(wav, threshold = 0.01) {
  # Detectar los silencios basados en la amplitud
  # Se considera silencio si la amplitud es menor que el umbral (threshold)
  silent_part <- silence(wav, threshold = threshold)
  
  # Eliminar los segmentos con silencio
  wav_no_silence <- cutw(wav, from = min(silent_part), to = max(silent_part), output = "Wave")
  
  return(wav_no_silence)
}

# Aplicar eliminación de silencios a cada audio
audio_no_silence <- lapply(audios, remove_silence)

# Aplicar la función a todos los audios y calcular los LPCCs
lpcc_results <- lapply(audio_no_silence, function(wav) {
  signal <- as.vector(wav@left)  # Convertir audio a vector (usando el canal 'left' para señales mono)
  calc_lpcc(signal, frame_size, overlap, order)
})

# Crear un ID para cada audio
audio_ids <- 1:length(audios)  # IDs de 1 a 70

# Crear el dataframe
lpcc_df <- data.frame(
  audio_id = audio_ids,  # Columna con los IDs
  lpcc_list = lpcc_results  # Columna con las listas de LPCCs
)

# Ver el dataframe resultante
head(lpcc_df)

```

```{r extraccion_caract_2}

# Inicializar una lista o una matriz para almacenar los LPCCs de cada audio
lpcc_resultados <- matrix(0, nrow = length(audios), ncol = 4)  # 4 coeficientes LPCC por audio

id <- 1
for (audio in audios) {
  # Calcular los LPCC para el audio actual
  # lpcc_result <- LPCC2(audio, wlen = audio@samp.rate * 0.02, order = 4)
  # Prueba con una ventana más grande, por ejemplo, 0.1 segundos
  lpcc_result <- LPCC2(audio, wlen = audio@samp.rate * 0.1, order = 4)

  
  # Obtener el primer LPCC calculado (puedes ajustar este índice si quieres algo diferente)
  lpcc_resultados[id, ] <- lpcc_result  # Suponiendo que cada resultado de LPCC2 es una lista
  
  id <- id + 1
}

# Guardar los LPCCs de todos los audios en df_caract
df_caract <- cbind(df_caract, lpcc_resultados)


```

```{r}


lpcc_c <- rep(0.0, length(audios))  
id <- 1

for (audio in audios) {
  
  lpcc_features <- LPCC(audio, audio@samp.rate * 0.02, order = 4)  
  
  # Para simplificar, vamos a calcular el promedio de los coeficientes LPCC para cada audio
  lpcc_c[id] <- mean(unlist(lpcc_features))  # Unimos la lista de LPCC y calculamos el promedio
  id <- id + 1
}

df_caract <- cbind(df_caract, lpcc_c)  # Añadir la nueva característica al dataframe


```

```{r}
df <- df_caract %>%
  left_join(meta_audios, by = "id_audio")  # Combinar los datos
# Reordenar los niveles de los locutores
df$locutor <- factor(df$locutor, levels = c("01", "02", "03", "05", "06","04" ,"07", "08", "09", "10"))
 

```

```{r}

ggplot(df, aes(x = locutor, y = ste_c, color = genero)) +
  geom_point(size = 3, alpha = 0.8) +  # Puntos
  geom_boxplot(aes(group = genero), alpha = 0.3) +  # Boxplots por género
  labs(
    title = "Short-Time Energy (STE) por Locutor y Género",
    x = "Locutor",
    y = "Short-Time Energy (STE)",
    color = "Género"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))




```



```{r}

ggplot(df, aes(x = ste_c, fill = genero)) +
  geom_density(alpha = 0.5) +  # Curvas de densidad
  labs(
    title = "Distribución de STE por Género",
    x = "Short-Time Energy (STE)",
    y = "Densidad",
    fill = "Género"
  ) +
  theme_minimal()

```

## Característica N

```{r extraccion_caract_n}
# WIP
```

# Algoritmos de ML

## Clustering

```{r algoritmos_clustering}
# WIP
```

## Clasificación

```{r algoritmos_clasificacion}
# WIP
```



